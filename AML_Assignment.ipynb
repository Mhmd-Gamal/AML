{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRy6zWS0NE0Y"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxRsXylP-XU1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX5R4a9GgESz",
        "outputId": "3ef64791-39f5-4f13-e226-2ab39a98bc17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data_transaction.zip\n",
            "  inflating: data_transaction.csv    \n",
            "Archive:  data_identity.zip\n",
            "  inflating: data_identity.csv       \n"
          ]
        }
      ],
      "source": [
        "#!unzip data_transaction.zip\n",
        "#!unzip data_identity.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rufzddlXhQA1"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_trans = pd.read_csv(\"data_transaction.csv\")\n",
        "data_identity = pd.read_csv(\"data_identity.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfHPdvLpducc",
        "outputId": "0cf05d84-2732-4678-c8d4-55a1e0e6b29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data transaction shape (590540, 394)\n",
            "data identity shape (144233, 41)\n"
          ]
        }
      ],
      "source": [
        "print(\"data transaction shape\", data_trans.shape)\n",
        "print(\"data identity shape\", data_identity.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0VC5Xv7S5dw"
      },
      "source": [
        "#### EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "RQKbMwzwhXQw",
        "outputId": "858ebd5a-7048-4eed-ac3d-6aa7514db419"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
              "0        2987000        0          86400            68.5         W  13926   \n",
              "1        2987001        0          86401            29.0         W   2755   \n",
              "2        2987002        0          86469            59.0         W   4663   \n",
              "3        2987003        0          86499            50.0         W  18132   \n",
              "4        2987004        0          86506            50.0         H   4497   \n",
              "\n",
              "   card2  card3       card4  card5  ...                id_31  id_32  \\\n",
              "0    NaN  150.0    discover  142.0  ...                  NaN    NaN   \n",
              "1  404.0  150.0  mastercard  102.0  ...                  NaN    NaN   \n",
              "2  490.0  150.0        visa  166.0  ...                  NaN    NaN   \n",
              "3  567.0  150.0  mastercard  117.0  ...                  NaN    NaN   \n",
              "4  514.0  150.0  mastercard  102.0  ...  samsung browser 6.2   32.0   \n",
              "\n",
              "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  \\\n",
              "0        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
              "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
              "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
              "3        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
              "4  2220x1080  match_status:2      T     F     T      T      mobile   \n",
              "\n",
              "                      DeviceInfo  \n",
              "0                            NaN  \n",
              "1                            NaN  \n",
              "2                            NaN  \n",
              "3                            NaN  \n",
              "4  SAMSUNG SM-G892A Build/NRD90M  \n",
              "\n",
              "[5 rows x 434 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da0bb2ba-db06-4c5b-90ca-baaaba5c8f66\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>...</th>\n",
              "      <th>id_31</th>\n",
              "      <th>id_32</th>\n",
              "      <th>id_33</th>\n",
              "      <th>id_34</th>\n",
              "      <th>id_35</th>\n",
              "      <th>id_36</th>\n",
              "      <th>id_37</th>\n",
              "      <th>id_38</th>\n",
              "      <th>DeviceType</th>\n",
              "      <th>DeviceInfo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>...</td>\n",
              "      <td>samsung browser 6.2</td>\n",
              "      <td>32.0</td>\n",
              "      <td>2220x1080</td>\n",
              "      <td>match_status:2</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>mobile</td>\n",
              "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 434 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da0bb2ba-db06-4c5b-90ca-baaaba5c8f66')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da0bb2ba-db06-4c5b-90ca-baaaba5c8f66 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da0bb2ba-db06-4c5b-90ca-baaaba5c8f66');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = data_trans.merge(data_identity,  on ='TransactionID', how='left')\n",
        "# Merge the tables \n",
        "#df = pd.merge(left=data_trans, right=data_identity, left_on='TransactionID', right_on='TransactionID')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyEzG77q0QX0"
      },
      "outputs": [],
      "source": [
        "del data_trans, data_identity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVC_TJsUiszx",
        "outputId": "bf8c3d3f-c875-46f5-cc08-ae86f732adaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 434)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df.shape "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "NS9fvcO0rJtJ",
        "outputId": "66598b63-2294-4349-b494-107dd0676dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       TransactionID        isFraud  TransactionDT  TransactionAmt  \\\n",
              "count   5.905400e+05  590540.000000   5.905400e+05   590540.000000   \n",
              "mean    3.282270e+06       0.034990   7.372311e+06      135.027176   \n",
              "std     1.704744e+05       0.183755   4.617224e+06      239.162522   \n",
              "min     2.987000e+06       0.000000   8.640000e+04        0.251000   \n",
              "25%     3.134635e+06       0.000000   3.027058e+06       43.321000   \n",
              "50%     3.282270e+06       0.000000   7.306528e+06       68.769000   \n",
              "75%     3.429904e+06       0.000000   1.124662e+07      125.000000   \n",
              "max     3.577539e+06       1.000000   1.581113e+07    31937.391000   \n",
              "\n",
              "               card1          card2          card3          card5  \\\n",
              "count  590540.000000  581607.000000  588975.000000  586281.000000   \n",
              "mean     9898.734658     362.555488     153.194925     199.278897   \n",
              "std      4901.170153     157.793246      11.336444      41.244453   \n",
              "min      1000.000000     100.000000     100.000000     100.000000   \n",
              "25%      6019.000000     214.000000     150.000000     166.000000   \n",
              "50%      9678.000000     361.000000     150.000000     226.000000   \n",
              "75%     14184.000000     512.000000     150.000000     226.000000   \n",
              "max     18396.000000     600.000000     231.000000     237.000000   \n",
              "\n",
              "               addr1          addr2  ...          id_17         id_18  \\\n",
              "count  524834.000000  524834.000000  ...  139369.000000  45113.000000   \n",
              "mean      290.733794      86.800630  ...     189.451377     14.237337   \n",
              "std       101.741072       2.690623  ...      30.375360      1.561302   \n",
              "min       100.000000      10.000000  ...     100.000000     10.000000   \n",
              "25%       204.000000      87.000000  ...     166.000000     13.000000   \n",
              "50%       299.000000      87.000000  ...     166.000000     15.000000   \n",
              "75%       330.000000      87.000000  ...     225.000000     15.000000   \n",
              "max       540.000000     102.000000  ...     229.000000     29.000000   \n",
              "\n",
              "               id_19          id_20        id_21        id_22        id_24  \\\n",
              "count  139318.000000  139261.000000  5159.000000  5169.000000  4747.000000   \n",
              "mean      353.128174     403.882666   368.269820    16.002708    12.800927   \n",
              "std       141.095343     152.160327   198.847038     6.897665     2.372447   \n",
              "min       100.000000     100.000000   100.000000    10.000000    11.000000   \n",
              "25%       266.000000     256.000000   252.000000    14.000000    11.000000   \n",
              "50%       341.000000     472.000000   252.000000    14.000000    11.000000   \n",
              "75%       427.000000     533.000000   486.500000    14.000000    15.000000   \n",
              "max       671.000000     661.000000   854.000000    44.000000    26.000000   \n",
              "\n",
              "             id_25        id_26         id_32  \n",
              "count  5132.000000  5163.000000  77586.000000  \n",
              "mean    329.608924   149.070308     26.508597  \n",
              "std      97.461089    32.101995      3.737502  \n",
              "min     100.000000   100.000000      0.000000  \n",
              "25%     321.000000   119.000000     24.000000  \n",
              "50%     321.000000   149.000000     24.000000  \n",
              "75%     371.000000   169.000000     32.000000  \n",
              "max     548.000000   216.000000     32.000000  \n",
              "\n",
              "[8 rows x 403 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c19043c-a43a-4af3-aea4-95a37b97dd09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card5</th>\n",
              "      <th>addr1</th>\n",
              "      <th>addr2</th>\n",
              "      <th>...</th>\n",
              "      <th>id_17</th>\n",
              "      <th>id_18</th>\n",
              "      <th>id_19</th>\n",
              "      <th>id_20</th>\n",
              "      <th>id_21</th>\n",
              "      <th>id_22</th>\n",
              "      <th>id_24</th>\n",
              "      <th>id_25</th>\n",
              "      <th>id_26</th>\n",
              "      <th>id_32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.905400e+05</td>\n",
              "      <td>590540.000000</td>\n",
              "      <td>5.905400e+05</td>\n",
              "      <td>590540.000000</td>\n",
              "      <td>590540.000000</td>\n",
              "      <td>581607.000000</td>\n",
              "      <td>588975.000000</td>\n",
              "      <td>586281.000000</td>\n",
              "      <td>524834.000000</td>\n",
              "      <td>524834.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>139369.000000</td>\n",
              "      <td>45113.000000</td>\n",
              "      <td>139318.000000</td>\n",
              "      <td>139261.000000</td>\n",
              "      <td>5159.000000</td>\n",
              "      <td>5169.000000</td>\n",
              "      <td>4747.000000</td>\n",
              "      <td>5132.000000</td>\n",
              "      <td>5163.000000</td>\n",
              "      <td>77586.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.282270e+06</td>\n",
              "      <td>0.034990</td>\n",
              "      <td>7.372311e+06</td>\n",
              "      <td>135.027176</td>\n",
              "      <td>9898.734658</td>\n",
              "      <td>362.555488</td>\n",
              "      <td>153.194925</td>\n",
              "      <td>199.278897</td>\n",
              "      <td>290.733794</td>\n",
              "      <td>86.800630</td>\n",
              "      <td>...</td>\n",
              "      <td>189.451377</td>\n",
              "      <td>14.237337</td>\n",
              "      <td>353.128174</td>\n",
              "      <td>403.882666</td>\n",
              "      <td>368.269820</td>\n",
              "      <td>16.002708</td>\n",
              "      <td>12.800927</td>\n",
              "      <td>329.608924</td>\n",
              "      <td>149.070308</td>\n",
              "      <td>26.508597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.704744e+05</td>\n",
              "      <td>0.183755</td>\n",
              "      <td>4.617224e+06</td>\n",
              "      <td>239.162522</td>\n",
              "      <td>4901.170153</td>\n",
              "      <td>157.793246</td>\n",
              "      <td>11.336444</td>\n",
              "      <td>41.244453</td>\n",
              "      <td>101.741072</td>\n",
              "      <td>2.690623</td>\n",
              "      <td>...</td>\n",
              "      <td>30.375360</td>\n",
              "      <td>1.561302</td>\n",
              "      <td>141.095343</td>\n",
              "      <td>152.160327</td>\n",
              "      <td>198.847038</td>\n",
              "      <td>6.897665</td>\n",
              "      <td>2.372447</td>\n",
              "      <td>97.461089</td>\n",
              "      <td>32.101995</td>\n",
              "      <td>3.737502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.987000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.640000e+04</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.134635e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.027058e+06</td>\n",
              "      <td>43.321000</td>\n",
              "      <td>6019.000000</td>\n",
              "      <td>214.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>204.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>266.000000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>252.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>321.000000</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>24.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.282270e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.306528e+06</td>\n",
              "      <td>68.769000</td>\n",
              "      <td>9678.000000</td>\n",
              "      <td>361.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>226.000000</td>\n",
              "      <td>299.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>341.000000</td>\n",
              "      <td>472.000000</td>\n",
              "      <td>252.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>321.000000</td>\n",
              "      <td>149.000000</td>\n",
              "      <td>24.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.429904e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.124662e+07</td>\n",
              "      <td>125.000000</td>\n",
              "      <td>14184.000000</td>\n",
              "      <td>512.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>226.000000</td>\n",
              "      <td>330.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>225.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>427.000000</td>\n",
              "      <td>533.000000</td>\n",
              "      <td>486.500000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>371.000000</td>\n",
              "      <td>169.000000</td>\n",
              "      <td>32.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.577539e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.581113e+07</td>\n",
              "      <td>31937.391000</td>\n",
              "      <td>18396.000000</td>\n",
              "      <td>600.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>237.000000</td>\n",
              "      <td>540.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>229.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>671.000000</td>\n",
              "      <td>661.000000</td>\n",
              "      <td>854.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>548.000000</td>\n",
              "      <td>216.000000</td>\n",
              "      <td>32.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 403 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c19043c-a43a-4af3-aea4-95a37b97dd09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c19043c-a43a-4af3-aea4-95a37b97dd09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c19043c-a43a-4af3-aea4-95a37b97dd09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIdjJLHakYbF",
        "outputId": "816f4eab-126c-4e55-d358-54e4221997d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 590540 entries, 0 to 590539\n",
            "Columns: 434 entries, TransactionID to DeviceInfo\n",
            "dtypes: float64(399), int64(4), object(31)\n",
            "memory usage: 1.9+ GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = df.select_dtypes(include=[np.float64]).columns\n",
        "df[cols] = df[cols].astype(np.float32)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16GnY9RxmS6f",
        "outputId": "ecf2f23e-471e-480c-ad00-674b0e5c2d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 590540 entries, 0 to 590539\n",
            "Columns: 434 entries, TransactionID to DeviceInfo\n",
            "dtypes: float32(399), int64(4), object(31)\n",
            "memory usage: 1.0+ GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(f'There are {df.isnull().any().sum()} columns in dataset with missing values.')"
      ],
      "metadata": {
        "id": "1qfLMZcwmS9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a2d0e3-9f3f-4bab-9592-32514aa5f0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 414 columns in dataset with missing values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find unique values\n",
        "one_value_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
        "print(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')\n"
      ],
      "metadata": {
        "id": "iv3gHERmmTAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07beb35-ed9a-499a-bbce-5d2facd7753f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 columns in train dataset with one unique value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the distribution for the features\n",
        "plt.hist(df['id_01'], bins=77);\n",
        "plt.title('Distribution of id_01 variable');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "UVxPxhk3q-z8",
        "outputId": "887801b9-8650-4c83-f7a1-c9aa8987df48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcwUlEQVR4nO3de5hV9X3v8fdHEDUqtzChBoiQhJoQzxOjE6UnTXPBwGDSYNvEaHtkYqmcREyT1POkmEtJNfZo08aENvGUBiqYRESTFJpgCBI9ac8pyngPGg8jXpgRZRQEI16C+Z4/1nd0sd0zswdm9sDweT3Pfmat7/qttX6LNezPXpc9SxGBmZkd2g4b6A6YmdnAcxiYmZnDwMzMHAZmZobDwMzMcBiYmRkOAzMzw2FgPZD0vyR9qY+W9QZJv5I0JMdvkfRnfbHsXN6Nkpr7anm9WO9XJD0p6fEq094t6YFu5r1a0lf6t4f9pze/H93tb0kTJYWkoX3bQ6uVw+AQJulhSc9JekbS05L+r6RPSHr59yIiPhERl9a4rNO7axMRj0bEMRHxUh/0/cuSvlOx/JkRsXR/l93LfrwBuAiYEhG/VTk9Iv49Ik7og/VMk/RLSbsl3Szp+NK0s3Lf7ZZ0y/6uqzdq/f2wA5/DwH4/Io4FjgcuB/4SWNzXKxnEn/jeADwVEdv6awWSxgA/AL4EjAZagOtKTbYDX6fYf3XTeYRng4PDwACIiJ0RsQr4GNAs6UTY+zSGpDGSfpRHEdsl/bukwyRdQ/Gm+G95GuhzpcP+OZIeBX7WxamAN0m6TdIuSSsljc51vVdSW7mPnUcfkpqAzwMfy/XdndNfPg2R/fqipEckbZO0TNKInNbZj2ZJj+Ypni909W8jaUTO35HL+2Iu/3RgLfD67MfVVebdazskvUPSHXk0dh1wZA275w+BjRFxfUQ8D3wZeLukt+S+uykiVgCP9bQgSfdL+lBpfGhu18k5fr2kxyXtlPRzSW8rtb1a0lWSVkt6Fnhfxe/HqPz96JC0I4fHV3Sh6v6u0s8RkhZL2iqpXcWpOIdPP3IY2F4i4jagDXh3lckX5bQGYCzFG3JExLnAoxRHGcdExN+W5nkP8FZgRhernA38KXAcsAdYWEMffwL8DXBdru/tVZp9PF/vA94IHAP8Y0Wb3wVOAKYBfyXprV2s8h+AEbmc92Sfz4uIm4CZwGPZj493129Jw4B/Ba6h+IR/PfBH3c2T3gbc3TkSEc8CD2a9t64FzimNzwCejIg7cvxGYDLwOuAO4LsV8/8xcBlwLPAfFdMOA/6F4ijzDcBzvPrfvNb9fXVOfzPwDmA60GfXl+zVHAZWzWMUb1aVfk3xn/j4iPh1ng/v6S8dfjkino2I57qYfk1E/CLf4L4EnNVHnwD/BPhaRGyOiF8BFwNnVxyV/HVEPBcRd1O82b4qVLIvZwMXR8QzEfEw8PfAufvQp6nA4cDX89/vBmBDDfMdA+ysqO2keEPure8BH5b0mhz/Y4qAACAiluR2vsArRyAjSvOvjIj/ExG/yaMUSvM+FRHfj4jdEfEMRWi8p2L9Pe5vSWOBM4DP5O/ONuBKiv1g/cRhYNWMozgPXemrQCvwU0mbJc2vYVlbejH9EYo3yzE19bJ7r8/llZc9lOKIplP57p/dFG+6lcZknyqXNW4f+9ReEaCPdNW45FfA8IracOCZ3nYgIlqB+4Hfz0D4MEVAIGmIpMslPShpF/BwzlbeH13uT0mvkfRPeSptF/BzYGTFm30t+/v4rG/NU5JPA/9EcbRi/cRhYHuR9E6KN7rKUwDkJ8aLIuKNFG8ifyFpWufkLhbZ05HDhNLwGyiOPp4EngU6P712fkJv6MVyH6N4Uykvew/wRA/zVXoy+1S5rPZeLgdgKzBOkiqW1ZONlI5aJB0NvCnr+6LzVNEs4L4MCCiOEmYBp1OcFpvYucrSvN39u19EcdrttIgYDvxelfm72t9lW4AXgDERMTJfwyNiX06LWY0cBgaApOF5YXE58J2IuLdKmw9JenO+me0EXgJ+k5OfoDin3lv/TdKU/JR6CXBD3nr6/4AjJX1Q0uHAF4EjSvM9AUxU6TbYCtcCn5U0SdIxvHKNYU9vOpd9WQFcJulYFbd0/gXwne7nrOo/KQLpzyUdLukPgVNrmO+HwImS/kjSkcBfAfdExC/h5U/0R1Ic+Rwm6cj8N+vKcopz8J8kjwrSsRRvwk9RBPHf9G7zOJbiOsHTeWF4QZU2Xe3vl0XEVuCnwN/n7+Vhkt4kqfKUk/Uhh4H9m6RnKD6NfQH4GnBeF20nAzdRnLb4T+BbEXFzTvufwBfzsP5/9GL911BcLHyc4s6aP4fi7ibgAuDbFJ/Cn6W4eN3p+vz5lKQ7eLUlueyfAw8BzwOf6kW/yj6V699MccT0vVx+r0TEixR3Bn2c4jTcxyhuGe1pvg6KC82XATuA09j7/Pm5FG/CV1Fc+H8O+OdulreVYv/9V/a+RXUZxambduA+YH1NG/aKrwNHUXzSXw/8pEqbqvu7itnAsOzHDuAGiutV1k/kJ52ZmZmPDMzMzGFgdiCQ9Pn84lrl68aB7psdGnyayMzMOGj/XsyYMWNi4sSJA90NM7ODxu233/5kRDRUm3bQhsHEiRNpaWkZ6G6YmR00JHX5JUdfMzAzM4eBmZk5DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM+Mg/gaymdnBaOL8H+81/vDlHxygnuzNRwZmZlZbGEj6rKSNkn4h6dp8rN4kSbdKapV0naRh2faIHG/N6RNLy7k46w9ImlGqN2WttcaHrJuZWR/qMQwkjaN4NF1jRJwIDKF45N4VwJUR8WaKx9LNyVnmADuyfmW2Q9KUnO9tQBPwrXx26xDgm8BMYApwTrY1M7M6qfU00VDgKElDKR6UvRV4P8VzSQGWAmfm8KwcJ6dPyweozwKWR8QLEfEQ0ErxMPBTgdaI2JzPiF2ebc3MrE56DIOIaAf+DniUIgR2ArcDT0fEnmzWBozL4XEUD1cnp+8EXluuV8zTVf1VJM2V1CKppaOjo5btMzOzGtRymmgUxSf1ScDrgaMpTvPUXUQsiojGiGhsaKj6fAYzM9sHtZwmOh14KCI6IuLXwA+AdwEj87QRwHigPYfbgQkAOX0E8FS5XjFPV3UzM6uTWsLgUWCqpNfkuf9pwH3AzcBHsk0zsDKHV+U4Of1nUTxoeRVwdt5tNAmYDNwGbAAm591JwyguMq/a/00zM7Na9fils4i4VdINwB3AHuBOYBHwY2C5pK9kbXHOshi4RlIrsJ3izZ2I2ChpBUWQ7AHmRcRLAJIuBNZQ3Km0JCI29t0mmplZT2r6BnJELAAWVJQ3U9wJVNn2eeCjXSznMuCyKvXVwOpa+mJmZn3P30A2MzOHgZmZOQzMzAyHgZmZ4TAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZjgMzMwMh4GZmeEwMDMzHAZmZobDwMzMcBiYmRk1hIGkEyTdVXrtkvQZSaMlrZW0KX+OyvaStFBSq6R7JJ1cWlZztt8kqblUP0XSvTnPwny8ppmZ1UmPYRARD0TESRFxEnAKsBv4ITAfWBcRk4F1OQ4wk+L5xpOBucBVAJJGUzwt7TSKJ6Qt6AyQbHN+ab6mPtk6MzOrSW9PE00DHoyIR4BZwNKsLwXOzOFZwLIorAdGSjoOmAGsjYjtEbEDWAs05bThEbE+IgJYVlqWmZnVQW/D4Gzg2hweGxFbc/hxYGwOjwO2lOZpy1p39bYq9VeRNFdSi6SWjo6OXnbdzMy6UnMYSBoGfBi4vnJafqKPPuxXVRGxKCIaI6KxoaGhv1dnZnbI6M2RwUzgjoh4IsefyFM85M9tWW8HJpTmG5+17urjq9TNzKxOehMG5/DKKSKAVUDnHUHNwMpSfXbeVTQV2Jmnk9YA0yWNygvH04E1OW2XpKl5F9Hs0rLMzKwOhtbSSNLRwAeA/14qXw6skDQHeAQ4K+urgTOAVoo7j84DiIjtki4FNmS7SyJiew5fAFwNHAXcmC8zM6uTmsIgIp4FXltRe4ri7qLKtgHM62I5S4AlVeotwIm19MXMzPqev4FsZmYOAzMzcxiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzNqDANJIyXdIOmXku6X9DuSRktaK2lT/hyVbSVpoaRWSfdIOrm0nOZsv0lSc6l+iqR7c56F+fhLMzOrk1qPDL4B/CQi3gK8HbgfmA+si4jJwLocB5gJTM7XXOAqAEmjgQXAacCpwILOAMk255fma9q/zTIzs97oMQwkjQB+D1gMEBEvRsTTwCxgaTZbCpyZw7OAZVFYD4yUdBwwA1gbEdsjYgewFmjKacMjYn0+MnNZaVlmZlYHtRwZTAI6gH+RdKekb0s6GhgbEVuzzePA2BweB2wpzd+Wte7qbVXqZmZWJ7WEwVDgZOCqiHgH8CyvnBICID/RR993b2+S5kpqkdTS0dHR36szMztk1BIGbUBbRNya4zdQhMMTeYqH/Lktp7cDE0rzj89ad/XxVeqvEhGLIqIxIhobGhpq6LqZmdWixzCIiMeBLZJOyNI04D5gFdB5R1AzsDKHVwGz866iqcDOPJ20BpguaVReOJ4OrMlpuyRNzbuIZpeWZWZmdTC0xnafAr4raRiwGTiPIkhWSJoDPAKclW1XA2cArcDubEtEbJd0KbAh210SEdtz+ALgauAo4MZ8mZlZndQUBhFxF9BYZdK0Km0DmNfFcpYAS6rUW4ATa+mLmZn1PX8D2czMHAZmZuYwMDMzHAZmZobDwMzMcBiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzIwaw0DSw5LulXSXpJasjZa0VtKm/Dkq65K0UFKrpHsknVxaTnO23ySpuVQ/JZffmvOqrzfUzMy61psjg/dFxEkR0fn4y/nAuoiYDKzLcYCZwOR8zQWugiI8gAXAacCpwILOAMk255fma9rnLTIzs17bn9NEs4ClObwUOLNUXxaF9cBISccBM4C1EbE9InYAa4GmnDY8Itbn85OXlZZlZmZ1UGsYBPBTSbdLmpu1sRGxNYcfB8bm8DhgS2netqx1V2+rUn8VSXMltUhq6ejoqLHrZmbWk6E1tvvdiGiX9DpgraRflidGREiKvu/e3iJiEbAIoLGxsd/XZ2Z2qKjpyCAi2vPnNuCHFOf8n8hTPOTPbdm8HZhQmn181rqrj69SNzOzOukxDCQdLenYzmFgOvALYBXQeUdQM7Ayh1cBs/OuoqnAzjydtAaYLmlUXjieDqzJabskTc27iGaXlmVmZnVQy2miscAP827PocD3IuInkjYAKyTNAR4Bzsr2q4EzgFZgN3AeQERsl3QpsCHbXRIR23P4AuBq4CjgxnyZmVmd9BgGEbEZeHuV+lPAtCr1AOZ1sawlwJIq9RbgxBr6a2Zm/cDfQDYzM4eBmZk5DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzMcBmZmhsPAzMxwGJiZGb0IA0lDJN0p6Uc5PknSrZJaJV0naVjWj8jx1pw+sbSMi7P+gKQZpXpT1lolze+7zTMzs1r05sjg08D9pfErgCsj4s3ADmBO1ucAO7J+ZbZD0hTgbOBtQBPwrQyYIcA3gZnAFOCcbGtmZnVSUxhIGg98EPh2jgt4P3BDNlkKnJnDs3KcnD4t288ClkfECxHxEMUzkk/NV2tEbI6IF4Hl2dbMzOqk1iODrwOfA36T468Fno6IPTneBozL4XHAFoCcvjPbv1yvmKer+qtImiupRVJLR0dHjV03M7Oe9BgGkj4EbIuI2+vQn25FxKKIaIyIxoaGhoHujpnZoDG0hjbvAj4s6QzgSGA48A1gpKSh+el/PNCe7duBCUCbpKHACOCpUr1TeZ6u6mZmVgc9HhlExMURMT4iJlJcAP5ZRPwJcDPwkWzWDKzM4VU5Tk7/WURE1s/Ou40mAZOB24ANwOS8O2lYrmNVn2ydmZnVpJYjg678JbBc0leAO4HFWV8MXCOpFdhO8eZORGyUtAK4D9gDzIuIlwAkXQisAYYASyJi4370y8zMeqlXYRARtwC35PBmijuBKts8D3y0i/kvAy6rUl8NrO5NX8zMrO/4G8hmZuYwMDMzh4GZmeEwMDMzHAZmZobDwMzMcBiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM6OGMJB0pKTbJN0taaOkv876JEm3SmqVdF0+spJ8rOV1Wb9V0sTSsi7O+gOSZpTqTVlrlTS/7zfTzMy6U8uRwQvA+yPi7cBJQJOkqcAVwJUR8WZgBzAn288BdmT9ymyHpCkUj8B8G9AEfEvSEElDgG8CM4EpwDnZ1szM6qTHMIjCr3L08HwF8H7ghqwvBc7M4Vk5Tk6fJklZXx4RL0TEQ0ArxWMzTwVaI2JzRLwILM+2ZmZWJzVdM8hP8HcB24C1wIPA0xGxJ5u0AeNyeBywBSCn7wReW65XzNNVvVo/5kpqkdTS0dFRS9fNzKwGNYVBRLwUEScB4yk+yb+lX3vVdT8WRURjRDQ2NDQMRBfMzAalXt1NFBFPAzcDvwOMlDQ0J40H2nO4HZgAkNNHAE+V6xXzdFU3M7M6qeVuogZJI3P4KOADwP0UofCRbNYMrMzhVTlOTv9ZRETWz867jSYBk4HbgA3A5Lw7aRjFReZVfbFxZmZWm6E9N+E4YGne9XMYsCIifiTpPmC5pK8AdwKLs/1i4BpJrcB2ijd3ImKjpBXAfcAeYF5EvAQg6UJgDTAEWBIRG/tsC83MrEc9hkFE3AO8o0p9M8X1g8r688BHu1jWZcBlVeqrgdU19NfMzPqBv4FsZmYOAzMzcxiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzNqe+zlBEk3S7pP0kZJn876aElrJW3Kn6OyLkkLJbVKukfSyaVlNWf7TZKaS/VTJN2b8yyUpP7YWDMzq66WI4M9wEURMQWYCsyTNAWYD6yLiMnAuhwHmEnxfOPJwFzgKijCA1gAnEbxhLQFnQGSbc4vzde0/5tmZma16jEMImJrRNyRw88A9wPjgFnA0my2FDgzh2cBy6KwHhgp6ThgBrA2IrZHxA5gLdCU04ZHxPqICGBZaVlmZlYHvbpmIGkixfOQbwXGRsTWnPQ4MDaHxwFbSrO1Za27eluVerX1z5XUIqmlo6OjN103M7Nu1BwGko4Bvg98JiJ2laflJ/ro4769SkQsiojGiGhsaGjo79WZmR0yhtbSSNLhFEHw3Yj4QZafkHRcRGzNUz3bst4OTCjNPj5r7cB7K+q3ZH18lfZmZoe8ifN/vNf4w5d/sF/WU8vdRAIWA/dHxNdKk1YBnXcENQMrS/XZeVfRVGBnnk5aA0yXNCovHE8H1uS0XZKm5rpml5ZlZmZ1UMuRwbuAc4F7Jd2Vtc8DlwMrJM0BHgHOymmrgTOAVmA3cB5ARGyXdCmwIdtdEhHbc/gC4GrgKODGfJmZWZ30GAYR8R9AV/f9T6vSPoB5XSxrCbCkSr0FOLGnvpiZWf/wN5DNzMxhYGZmDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZjgMzMwMh4GZmeEwMDMzHAZmZkZtj71cImmbpF+UaqMlrZW0KX+OyrokLZTUKukeSSeX5mnO9pskNZfqp0i6N+dZmI++NDM7YE2c/+OXX4NFLUcGVwNNFbX5wLqImAysy3GAmcDkfM0FroIiPIAFwGnAqcCCzgDJNueX5qtcl5nZXspvxoPpDXkg9RgGEfFzYHtFeRawNIeXAmeW6suisB4YKek4YAawNiK2R8QOYC3QlNOGR8T6fFzmstKyzMysTvb1msHYiNiaw48DY3N4HLCl1K4ta93V26rUzcysjvb7AnJ+oo8+6EuPJM2V1CKppaOjox6rNDM7JOxrGDyRp3jIn9uy3g5MKLUbn7Xu6uOr1KuKiEUR0RgRjQ0NDfvYdTMzq7SvYbAK6LwjqBlYWarPzruKpgI783TSGmC6pFF54Xg6sCan7ZI0Ne8iml1alpmZ1cnQnhpIuhZ4LzBGUhvFXUGXAyskzQEeAc7K5quBM4BWYDdwHkBEbJd0KbAh210SEZ0XpS+guGPpKODGfJmZWR31GAYRcU4Xk6ZVaRvAvC6WswRYUqXeApzYUz/MzKz/+BvIZmbmMDAzM4eBmZnhMDAzMxwGZmaGw8DMzKjh1lIzs8Gm8i+dPnz5BweoJwcOHxmYmZmPDMys/vzJ/MDjIwMzM3MYmJmZw8DMzHAYmJkZDgMzM8N3E5kdMHyHjQ0kHxmYmZmPDMwGi/05svBRiR0wYSCpCfgGMAT4dkRc3l/r8i++mdneDogwkDQE+CbwAaAN2CBpVUTcN7A9M6vdofQh41Da1kPFAREGwKlAa0RsBpC0HJgFHBBhcDAffg/0+utpf7f1UPq3Mquk4hn2A9wJ6SNAU0T8WY6fC5wWERdWtJsLzM3RE4AH9nGVY4An93Heg5W3efA71LYXvM29dXxENFSbcKAcGdQkIhYBi/Z3OZJaIqKxD7p00PA2D36H2vaCt7kvHSi3lrYDE0rj47NmZmZ1cKCEwQZgsqRJkoYBZwOrBrhPZmaHjAPiNFFE7JF0IbCG4tbSJRGxsR9Xud+nmg5C3ubB71DbXvA295kD4gKymZkNrAPlNJGZmQ0gh4GZmQ3uMJD0UUkbJf1GUmPFtIsltUp6QNKMUr0pa62S5te/131H0kmS1ku6S1KLpFOzLkkLcxvvkXTyQPe1L0n6lKRf5r7/21K96j4fLCRdJCkkjcnxQbufJX019/E9kn4oaWRp2qDdz/36/hQRg/YFvJXiy2m3AI2l+hTgbuAIYBLwIMWF6yE5/EZgWLaZMtDbsR/b/1NgZg6fAdxSGr4REDAVuHWg+9qH2/w+4CbgiBx/XXf7fKD724fbPYHiBoxHgDGHwH6eDgzN4SuAKwb7fu7v96dBfWQQEfdHRLVvKc8ClkfECxHxENBK8ScxXv6zGBHxItD5ZzEOVgEMz+ERwGM5PAtYFoX1wEhJxw1EB/vBJ4HLI+IFgIjYlvWu9vlgcSXwOYp93mnQ7ueI+GlE7MnR9RTfTYLBvZ/79f1pUIdBN8YBW0rjbVnrqn6w+gzwVUlbgL8DLs76YNvOst8G3i3pVkn/W9I7sz5ot1nSLKA9Iu6umDRot7nCn1IcAcHg3uZ+3bYD4nsG+0PSTcBvVZn0hYhYWe/+1Ft32w9MAz4bEd+XdBawGDi9nv3rDz1s81BgNMVpkXcCKyS9sY7d6xc9bPPnKU6bDCq1/N+W9AVgD/DdevZtMDrowyAi9uXNrbs/f3FQ/VmM7rZf0jLg0zl6PfDtHD6o//xHD9v8SeAHUZxkvU3Sbyj+sNeg3GZJ/4Xi3PjdkqDYrjvyZoFBuc2dJH0c+BAwLfc3HOTb3IN+3bZD9TTRKuBsSUdImgRMBm5j8P1ZjMeA9+Tw+4FNObwKmJ13m0wFdkbE1oHoYD/4V4qLyEj6bYoLbU/S9T4/qEXEvRHxuoiYGBETKU4dnBwRjzOI93M+DOtzwIcjYndp0qDcz6lf358O+iOD7kj6A+AfgAbgx5LuiogZEbFR0gqK5yXsAeZFxEs5Tz3/LEZ/Ox/4hqShwPO88ue/V1PcadIK7AbOG5ju9YslwBJJvwBeBJrzU2OX+3wQG8z7+R8p7hham0dE6yPiE9393z7YRT//2R7/OQozMztkTxOZmVmJw8DMzBwGZmbmMDAzMxwGZmaGw8DMzHAYmJkZ8P8Bmlm2Nm4zlj8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** id_01 has 77 unique non-positive values with skeweness to 0."
      ],
      "metadata": {
        "id": "ZIEyr8ATsj4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['id_03'].value_counts(dropna=False, normalize=True).head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeeh_53Mq-3M",
        "outputId": "e98af3db-c002-461a-a1c1-3d4ab5263b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NaN    0.887689\n",
              "0.0    0.108211\n",
              "1.0    0.001461\n",
              "3.0    0.001131\n",
              "2.0    0.000713\n",
              "Name: id_03, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** id_03 has 88% of missing values and 98% of values are either missing or equal to 0."
      ],
      "metadata": {
        "id": "5YZ4_qrPtJRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['id_11'].value_counts(dropna=False, normalize=True).head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe7C1joJq-50",
        "outputId": "ef3af2c4-ebe3-4ab0-acab-57476bbed32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NaN           0.761273\n",
              "100.000000    0.225492\n",
              "95.080002     0.002085\n",
              "95.160004     0.001277\n",
              "97.120003     0.000745\n",
              "Name: id_11, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation** 22% of values in id_11 are equal to 100and 76% are missing."
      ],
      "metadata": {
        "id": "bjAT_f_ptfRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df['id_07']);\n",
        "plt.title('Distribution of id_07 variable');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "nhN6ihqaq-88",
        "outputId": "9365e805-f946-478d-d794-6e74f364b393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D1KecIdzq-_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HYoDbRM4q_C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PcNmxrqtq_ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m_ufdOGwq_jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-processing"
      ],
      "metadata": {
        "id": "6KgPIAN4vS-3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5SiRlF-gES2"
      },
      "outputs": [],
      "source": [
        "list_cat = list(df.select_dtypes('object').columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ5hK2cUgES3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9ffb00-4a40-4da5-d325-1b6b5c5b3576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(590540, 434)\n"
          ]
        }
      ],
      "source": [
        "# Convert the categorical data\n",
        "\n",
        "for col in list_cat:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        le.fit(list(df[col].astype(str).values))\n",
        "        df[col] = le.transform(list(df[col].astype(str).values))\n",
        "\n",
        "#df = pd.get_dummies(df)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_-5DGD49qEU"
      },
      "outputs": [],
      "source": [
        "y = df['isFraud'].to_numpy()\n",
        "df.drop('isFraud', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhSCJWzmdHHk"
      },
      "outputs": [],
      "source": [
        "df_cleared = df.notna().astype(\"int\").to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGmm9IRj428N"
      },
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "mask = sc.fit_transform(df.fillna(0).to_numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot79D8Yp7kk3"
      },
      "outputs": [],
      "source": [
        "full_data = np.hstack([mask, df_cleared])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del df, mask, df_cleared"
      ],
      "metadata": {
        "id": "ldz1uY-C-9rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKUybr6U7knw",
        "outputId": "f0862e46-d00c-461b-aa4e-79819773d957"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(590540, 866)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "full_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_data_shape = full_data.shape[1]"
      ],
      "metadata": {
        "id": "6rKjQHaS_cYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gr80_ZpWMoS"
      },
      "outputs": [],
      "source": [
        "#Splitting \n",
        "X_train, X_test, y_train, y_test = train_test_split( full_data, y, test_size=0.2, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del full_data"
      ],
      "metadata": {
        "id": "asRbTUVmJDK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB6K0Xym-_ul",
        "outputId": "65da1b74-1f33-4022-a6e4-582d8db14011"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(472432, 866)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn1sw4L-zq-F"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wThkLZDQLwEi"
      },
      "source": [
        "#### Define the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipNOkla4mWSc"
      },
      "source": [
        "#### Undercomplete AE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D93zMaA3LW-g"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pxUNj0TgES6"
      },
      "outputs": [],
      "source": [
        "## Undercomplete\n",
        "\n",
        "class autoencoder(nn.Module):\n",
        "    def __init__(self, input_size, latent_dim):\n",
        "      super(autoencoder, self).__init__()\n",
        "      # Step 1 : Define the encoder \n",
        "      # Step 2 : Define the decoder\n",
        "      # Step 3 : Initialize the weights (optional)\n",
        "      self.encoder = nn.Sequential(\n",
        "          nn.Linear(input_size, input_size//2),\n",
        "          nn.ReLU(True),\n",
        "          nn.Linear(input_size//2, input_size//3),\n",
        "          nn.Linear(input_size//3, input_size//4),\n",
        "          nn.Tanh(),\n",
        "          nn.Linear(input_size//4, latent_dim)\n",
        "      )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(latent_dim, input_size//4),\n",
        "          nn.ReLU(True),\n",
        "          nn.Linear(input_size//4, input_size//3),\n",
        "          nn.Tanh(),\n",
        "          nn.Linear(input_size//3, input_size//2)\n",
        "      )\n",
        "      self.encoder.apply(self.__init_weights)\n",
        "      self.decoder.apply(self.__init_weights)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      # Step 1: Pass the input through encoder to get latent representation\n",
        "      # Step 2: Take latent representation and pass through decoder\n",
        "      x = self.encoder(x)\n",
        "      x = self.decoder(x)\n",
        "      return x\n",
        "        \n",
        "    \n",
        "    def encode(self,input):\n",
        "      #Step 1: Pass the input through the encoder to get latent representation\n",
        "      return self.encoder(input)\n",
        "    \n",
        "    def decode(self, input):\n",
        "      return self.decoder(input)\n",
        "    \n",
        "    def __init_weights(self,m):\n",
        "      #Init the weights (optional)\n",
        "      if type(m) == nn.Linear:\n",
        "          torch.nn.init.xavier_uniform_(m.weight)\n",
        "          m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHBtiF5fgES7",
        "outputId": "669d73ef-8d00-47c7-dfd8-bc39d722de62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "autoencoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Linear(in_features=866, out_features=433, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=433, out_features=288, bias=True)\n",
            "    (3): Linear(in_features=288, out_features=216, bias=True)\n",
            "    (4): Tanh()\n",
            "    (5): Linear(in_features=216, out_features=50, bias=True)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=216, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=216, out_features=288, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=288, out_features=433, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define training parameters\n",
        "\n",
        "min_lose = 10\n",
        "batchSize = 512\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n",
        "latent_size = 50\n",
        "###sample = torch.randn((batchSize,1,64))\n",
        "AE = autoencoder(full_data_shape , latent_size).to(device)\n",
        "print(AE)\n",
        "# print(summary(AE,input_size=(1, 64)))\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(AE.parameters(),lr=learning_rate)\n",
        "\n",
        "# create DataLoaders\n",
        "dataLoader_train = DataLoader(TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train)), batch_size=batchSize, shuffle=True)\n",
        "dataLoader_test = DataLoader(TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test)), batch_size=batchSize, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFWMqePLgES7",
        "outputId": "d7422688-b548-47cb-a206-c18f679bc0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/50], train_loss:0.3417\n",
            "epoch [2/50], train_loss:0.2561\n",
            "epoch [3/50], train_loss:0.2330\n",
            "epoch [4/50], train_loss:0.2171\n",
            "epoch [5/50], train_loss:0.2042\n",
            "epoch [6/50], train_loss:0.1964\n",
            "epoch [7/50], train_loss:0.1887\n",
            "epoch [8/50], train_loss:0.2966\n",
            "epoch [9/50], train_loss:0.2755\n",
            "epoch [10/50], train_loss:0.2484\n",
            "epoch [11/50], train_loss:0.2485\n",
            "epoch [12/50], train_loss:0.2575\n",
            "epoch [13/50], train_loss:0.2255\n",
            "epoch [14/50], train_loss:0.2259\n",
            "epoch [15/50], train_loss:0.2245\n",
            "epoch [16/50], train_loss:0.2207\n",
            "epoch [17/50], train_loss:0.2113\n",
            "epoch [18/50], train_loss:0.2252\n",
            "epoch [19/50], train_loss:0.2321\n",
            "epoch [20/50], train_loss:0.2214\n",
            "epoch [21/50], train_loss:0.2199\n",
            "epoch [22/50], train_loss:0.2152\n",
            "epoch [23/50], train_loss:0.2267\n",
            "epoch [24/50], train_loss:0.2237\n",
            "epoch [25/50], train_loss:0.2070\n",
            "epoch [26/50], train_loss:0.2027\n",
            "epoch [27/50], train_loss:0.2132\n",
            "epoch [28/50], train_loss:0.2103\n",
            "epoch [29/50], train_loss:0.2043\n",
            "epoch [30/50], train_loss:0.2089\n",
            "epoch [31/50], train_loss:0.2052\n",
            "epoch [32/50], train_loss:0.2096\n",
            "epoch [33/50], train_loss:0.2059\n",
            "epoch [34/50], train_loss:0.2165\n",
            "epoch [35/50], train_loss:0.2260\n",
            "epoch [36/50], train_loss:0.2054\n",
            "epoch [37/50], train_loss:0.2110\n",
            "epoch [38/50], train_loss:0.2118\n",
            "epoch [39/50], train_loss:0.2077\n",
            "epoch [40/50], train_loss:0.2033\n",
            "epoch [41/50], train_loss:0.2015\n",
            "epoch [42/50], train_loss:0.2023\n",
            "epoch [43/50], train_loss:0.2138\n",
            "epoch [44/50], train_loss:0.2132\n",
            "epoch [45/50], train_loss:0.2174\n",
            "epoch [46/50], train_loss:0.2055\n",
            "epoch [47/50], train_loss:0.2085\n",
            "epoch [48/50], train_loss:0.2050\n",
            "epoch [49/50], train_loss:0.2020\n",
            "epoch [50/50], train_loss:0.2068\n"
          ]
        }
      ],
      "source": [
        "## Trainning the Autoencoder\n",
        "ind = full_data_shape//2\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  for X in dataLoader_train:\n",
        "    X = X[0].to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward\n",
        "    output = AE(X)\n",
        "    loss = criterion(output * X[:, ind:], X[:, :ind])\n",
        "    #print(output.dtype)\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  # log\n",
        "\n",
        "  print('epoch [{}/{}], train_loss:{:.4f}'.format(epoch + 1, num_epochs, epoch_loss / len(dataLoader_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKcuwGy6gES7",
        "outputId": "7205aa58-d5a3-4a6e-e778-a0b94dae98ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/50], test_loss:0.2049\n",
            "epoch [2/50], test_loss:0.2049\n",
            "epoch [3/50], test_loss:0.2049\n",
            "epoch [4/50], test_loss:0.2049\n",
            "epoch [5/50], test_loss:0.2049\n",
            "epoch [6/50], test_loss:0.2050\n",
            "epoch [7/50], test_loss:0.2050\n",
            "epoch [8/50], test_loss:0.2050\n",
            "epoch [9/50], test_loss:0.2050\n",
            "epoch [10/50], test_loss:0.2049\n",
            "epoch [11/50], test_loss:0.2049\n",
            "epoch [12/50], test_loss:0.2049\n",
            "epoch [13/50], test_loss:0.2049\n",
            "epoch [14/50], test_loss:0.2052\n",
            "epoch [15/50], test_loss:0.2049\n",
            "epoch [16/50], test_loss:0.2049\n",
            "epoch [17/50], test_loss:0.2050\n",
            "epoch [18/50], test_loss:0.2049\n",
            "epoch [19/50], test_loss:0.2049\n",
            "epoch [20/50], test_loss:0.2049\n",
            "epoch [21/50], test_loss:0.2050\n",
            "epoch [22/50], test_loss:0.2050\n",
            "epoch [23/50], test_loss:0.2049\n",
            "epoch [24/50], test_loss:0.2050\n",
            "epoch [25/50], test_loss:0.2050\n",
            "epoch [26/50], test_loss:0.2049\n",
            "epoch [27/50], test_loss:0.2049\n",
            "epoch [28/50], test_loss:0.2049\n",
            "epoch [29/50], test_loss:0.2049\n",
            "epoch [30/50], test_loss:0.2051\n",
            "epoch [31/50], test_loss:0.2050\n",
            "epoch [32/50], test_loss:0.2049\n",
            "epoch [33/50], test_loss:0.2051\n",
            "epoch [34/50], test_loss:0.2049\n",
            "epoch [35/50], test_loss:0.2051\n",
            "epoch [36/50], test_loss:0.2051\n",
            "epoch [37/50], test_loss:0.2049\n",
            "epoch [38/50], test_loss:0.2052\n",
            "epoch [39/50], test_loss:0.2049\n",
            "epoch [40/50], test_loss:0.2049\n",
            "epoch [41/50], test_loss:0.2050\n",
            "epoch [42/50], test_loss:0.2049\n",
            "epoch [43/50], test_loss:0.2050\n",
            "epoch [44/50], test_loss:0.2049\n",
            "epoch [45/50], test_loss:0.2050\n",
            "epoch [46/50], test_loss:0.2050\n",
            "epoch [47/50], test_loss:0.2049\n",
            "epoch [48/50], test_loss:0.2049\n",
            "epoch [49/50], test_loss:0.2049\n",
            "epoch [50/50], test_loss:0.2052\n"
          ]
        }
      ],
      "source": [
        "# Test \n",
        "min_undercomplete_loss = 10\n",
        "minimum_loss_un = AE.state_dict()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  with torch.no_grad():\n",
        "    loss = 0\n",
        "    for X in dataLoader_test:\n",
        "      X = X[0].to(device)\n",
        "      \n",
        "      output = AE(X)\n",
        "      loss += criterion(output * X[:, ind:], X[:, :ind]).item()\n",
        "    res = loss / len(dataLoader_test)\n",
        "    if res < min_undercomplete_loss:\n",
        "        min_undercomplete_loss = res\n",
        "        minimum_loss_un = AE.state_dict()\n",
        "    else: \n",
        "        AE.load_state_dict(minimum_loss_un)\n",
        "    print('epoch [{}/{}], test_loss:{:.4f}'.format(epoch + 1, num_epochs, res ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(minimum_loss_un, \"minimum_loss_undercomplete\")\n"
      ],
      "metadata": {
        "id": "aJeBgcYLLnxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO4NkFdnmNKn"
      },
      "source": [
        "#### Regularized AE "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y_pVB8mxAFs"
      },
      "source": [
        "**Sparse autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igWRw3JqgES8"
      },
      "outputs": [],
      "source": [
        "def regLoss(data):\n",
        "    AE_children = list(AE.children())\n",
        "    l = 0\n",
        "    v = data\n",
        "    for x in range(len(AE_children)):\n",
        "        v = F.relu((AE_children[x](v)))\n",
        "        l += torch.mean(torch.abs(v))\n",
        "    return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDTj3ryXgES8",
        "outputId": "1487d63e-0361-4f16-8560-42140edbb2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/50], train_loss:0.2614\n",
            "epoch [2/50], train_loss:0.2200\n",
            "epoch [3/50], train_loss:0.2228\n",
            "epoch [4/50], train_loss:0.2150\n",
            "epoch [5/50], train_loss:0.2216\n",
            "epoch [6/50], train_loss:0.2142\n",
            "epoch [7/50], train_loss:0.2211\n",
            "epoch [8/50], train_loss:0.2176\n",
            "epoch [9/50], train_loss:0.2144\n",
            "epoch [10/50], train_loss:0.2167\n",
            "epoch [11/50], train_loss:0.2146\n",
            "epoch [12/50], train_loss:0.2149\n",
            "epoch [13/50], train_loss:0.2210\n",
            "epoch [14/50], train_loss:0.2247\n",
            "epoch [15/50], train_loss:0.2140\n",
            "epoch [16/50], train_loss:0.2133\n",
            "epoch [17/50], train_loss:0.2116\n",
            "epoch [18/50], train_loss:0.2112\n",
            "epoch [19/50], train_loss:0.2291\n",
            "epoch [20/50], train_loss:0.2189\n",
            "epoch [21/50], train_loss:0.2185\n",
            "epoch [22/50], train_loss:0.2204\n",
            "epoch [23/50], train_loss:0.2222\n",
            "epoch [24/50], train_loss:0.2138\n",
            "epoch [25/50], train_loss:0.2072\n",
            "epoch [26/50], train_loss:0.2091\n",
            "epoch [27/50], train_loss:0.2058\n",
            "epoch [28/50], train_loss:0.2046\n",
            "epoch [29/50], train_loss:0.2089\n",
            "epoch [30/50], train_loss:0.2202\n",
            "epoch [31/50], train_loss:0.2110\n",
            "epoch [32/50], train_loss:0.2053\n",
            "epoch [33/50], train_loss:0.2122\n",
            "epoch [34/50], train_loss:0.2196\n",
            "epoch [35/50], train_loss:0.2124\n",
            "epoch [36/50], train_loss:0.2047\n",
            "epoch [37/50], train_loss:0.2066\n",
            "epoch [38/50], train_loss:0.2150\n",
            "epoch [39/50], train_loss:0.2082\n",
            "epoch [40/50], train_loss:0.2111\n",
            "epoch [41/50], train_loss:0.2142\n",
            "epoch [42/50], train_loss:0.2171\n",
            "epoch [43/50], train_loss:0.2172\n",
            "epoch [44/50], train_loss:0.2124\n",
            "epoch [45/50], train_loss:0.2184\n",
            "epoch [46/50], train_loss:0.2142\n",
            "epoch [47/50], train_loss:0.2095\n",
            "epoch [48/50], train_loss:0.2095\n",
            "epoch [49/50], train_loss:0.2093\n",
            "epoch [50/50], train_loss:0.2206\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  for X in dataLoader_train:\n",
        "    X = X[0].to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward\n",
        "    output = AE(X)\n",
        "    loss = criterion(output * X[:, ind:], X[:, :ind])\n",
        "    l1_loss = regLoss(X)\n",
        "    loss = loss +  l1_loss\n",
        "    #print(output.dtype)\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  # log\n",
        "  print('epoch [{}/{}], train_loss:{:.4f}'.format(epoch + 1, num_epochs, epoch_loss / len(dataLoader_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekwImrhdgES9",
        "outputId": "5418391f-d684-43da-d76e-170e7eea82fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch [1/50], test_loss:0.2000\n",
            "epoch [2/50], test_loss:0.1999\n",
            "epoch [3/50], test_loss:0.1997\n",
            "epoch [4/50], test_loss:0.1999\n",
            "epoch [5/50], test_loss:0.1999\n",
            "epoch [6/50], test_loss:0.2027\n",
            "epoch [7/50], test_loss:0.1998\n",
            "epoch [8/50], test_loss:0.1997\n",
            "epoch [9/50], test_loss:0.2000\n",
            "epoch [10/50], test_loss:0.1999\n",
            "epoch [11/50], test_loss:0.1999\n",
            "epoch [12/50], test_loss:0.2003\n",
            "epoch [13/50], test_loss:0.1998\n",
            "epoch [14/50], test_loss:0.2003\n",
            "epoch [15/50], test_loss:0.1999\n",
            "epoch [16/50], test_loss:0.1998\n",
            "epoch [17/50], test_loss:0.2000\n",
            "epoch [18/50], test_loss:0.1999\n",
            "epoch [19/50], test_loss:0.2008\n",
            "epoch [20/50], test_loss:0.1997\n",
            "epoch [21/50], test_loss:0.1998\n",
            "epoch [22/50], test_loss:0.1997\n",
            "epoch [23/50], test_loss:0.2008\n",
            "epoch [24/50], test_loss:0.1997\n",
            "epoch [25/50], test_loss:0.2009\n",
            "epoch [26/50], test_loss:0.1998\n",
            "epoch [27/50], test_loss:0.1998\n",
            "epoch [28/50], test_loss:0.1999\n",
            "epoch [29/50], test_loss:0.1998\n",
            "epoch [30/50], test_loss:0.2010\n",
            "epoch [31/50], test_loss:0.1998\n",
            "epoch [32/50], test_loss:0.1998\n",
            "epoch [33/50], test_loss:0.1998\n",
            "epoch [34/50], test_loss:0.1998\n",
            "epoch [35/50], test_loss:0.1999\n",
            "epoch [36/50], test_loss:0.1999\n",
            "epoch [37/50], test_loss:0.2000\n",
            "epoch [38/50], test_loss:0.2009\n",
            "epoch [39/50], test_loss:0.1998\n",
            "epoch [40/50], test_loss:0.2003\n",
            "epoch [41/50], test_loss:0.1996\n",
            "epoch [42/50], test_loss:0.1999\n",
            "epoch [43/50], test_loss:0.1996\n",
            "epoch [44/50], test_loss:0.1999\n",
            "epoch [45/50], test_loss:0.1997\n",
            "epoch [46/50], test_loss:0.1998\n",
            "epoch [47/50], test_loss:0.1998\n",
            "epoch [48/50], test_loss:0.1997\n",
            "epoch [49/50], test_loss:0.1999\n",
            "epoch [50/50], test_loss:0.2010\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "min_reg_ae = 10\n",
        "minimum_loss_re = AE.state_dict()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  with torch.no_grad():\n",
        "    loss = 0\n",
        "    for X in dataLoader_test:\n",
        "      X = X[0].to(device)\n",
        "      \n",
        "      output = AE(X)\n",
        "      loss += criterion(output * X[:, ind:], X[:, :ind]).item()\n",
        "    \n",
        "    print('epoch [{}/{}], test_loss:{:.4f}'.format(epoch + 1, num_epochs, loss / len(dataLoader_test)))\n",
        "    if (loss / len(dataLoader_test)) < min_reg_ae:\n",
        "      min_reg_ae = (loss / len(dataLoader_test))\n",
        "      minimum_loss_re = AE.state_dict()\n",
        "    else: \n",
        "        AE.load_state_dict(minimum_loss_re)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(minimum_loss_re, \"minimum_loss_Regularized\")\n"
      ],
      "metadata": {
        "id": "IdiZckUenrZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw7_yeB4yZ8j"
      },
      "source": [
        "#### Variational AE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MfpfuIKgES_"
      },
      "outputs": [],
      "source": [
        "# VAE model\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim= full_data.shape[1], h_dim= 389, z_dim=5):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, h_dim)\n",
        "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc5 = nn.Linear(h_dim, input_dim//2)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h), self.fc3(h)\n",
        "    \n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return self.fc5(h)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        return x_reconst, mu, log_var\n",
        "\n",
        "model = VAE().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCJKkrPT3k2K",
        "outputId": "2c693135-9fac-4866-9c15-0d8343a2116c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAE(\n",
            "  (fc1): Linear(in_features=778, out_features=389, bias=True)\n",
            "  (fc2): Linear(in_features=389, out_features=5, bias=True)\n",
            "  (fc3): Linear(in_features=389, out_features=5, bias=True)\n",
            "  (fc4): Linear(in_features=5, out_features=389, bias=True)\n",
            "  (fc5): Linear(in_features=389, out_features=389, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRl4BpvQgES_",
        "outputId": "7510e83a-030b-4777-8553-72e73dce6504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch [1/50], train_loss:32.5471\n",
            "epoch [2/50], train_loss:25.9521\n",
            "epoch [3/50], train_loss:32.1549\n",
            "epoch [4/50], train_loss:32.6807\n",
            "epoch [5/50], train_loss:20.9882\n",
            "epoch [6/50], train_loss:23.0161\n",
            "epoch [7/50], train_loss:28.7523\n",
            "epoch [8/50], train_loss:31.4485\n",
            "epoch [9/50], train_loss:29.0393\n",
            "epoch [10/50], train_loss:19.8426\n",
            "epoch [11/50], train_loss:35.7764\n",
            "epoch [12/50], train_loss:25.0710\n",
            "epoch [13/50], train_loss:29.5866\n",
            "epoch [14/50], train_loss:34.3789\n",
            "epoch [15/50], train_loss:35.8678\n",
            "epoch [16/50], train_loss:203.2830\n",
            "epoch [17/50], train_loss:27.0489\n",
            "epoch [18/50], train_loss:28.3591\n",
            "epoch [19/50], train_loss:24.0067\n",
            "epoch [20/50], train_loss:22.2269\n",
            "epoch [21/50], train_loss:24.6657\n",
            "epoch [22/50], train_loss:31.5977\n",
            "epoch [23/50], train_loss:34.3280\n",
            "epoch [24/50], train_loss:27.1938\n",
            "epoch [25/50], train_loss:29.4029\n",
            "epoch [26/50], train_loss:25.3846\n",
            "epoch [27/50], train_loss:24.0688\n",
            "epoch [28/50], train_loss:23.4872\n",
            "epoch [29/50], train_loss:26.7886\n",
            "epoch [30/50], train_loss:27.1361\n",
            "epoch [31/50], train_loss:30.1381\n",
            "epoch [32/50], train_loss:28.9193\n",
            "epoch [33/50], train_loss:23.7328\n",
            "epoch [34/50], train_loss:32.2003\n",
            "epoch [35/50], train_loss:39.0054\n",
            "epoch [36/50], train_loss:32.1607\n",
            "epoch [37/50], train_loss:28.6720\n",
            "epoch [38/50], train_loss:36.0067\n",
            "epoch [39/50], train_loss:33.5951\n",
            "epoch [40/50], train_loss:23.8801\n",
            "epoch [41/50], train_loss:24.8588\n",
            "epoch [42/50], train_loss:21.9408\n",
            "epoch [43/50], train_loss:32.6010\n",
            "epoch [44/50], train_loss:30.9768\n",
            "epoch [45/50], train_loss:25.2515\n",
            "epoch [46/50], train_loss:30.1901\n",
            "epoch [47/50], train_loss:29.1237\n",
            "epoch [48/50], train_loss:27.7482\n",
            "epoch [49/50], train_loss:29.9767\n",
            "epoch [50/50], train_loss:32.8907\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0.0\n",
        "  for X in dataLoader_train:\n",
        "    X = X[0].to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward\n",
        "    x_reconst, mu, log_var = model(X)\n",
        "    loss = criterion(x_reconst*X[:, ind:], X[:, :ind])\n",
        "    kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    loss += kl_div\n",
        "\n",
        "    #print(output.dtype)\n",
        "    # backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "  # log\n",
        "  print('epoch [{}/{}], train_loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97ER0q5EgETA",
        "outputId": "5be2b357-323c-4627-d0fe-564ddfd81f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch [1/50], test_loss:0.9592\n",
            "epoch [2/50], test_loss:0.9570\n",
            "epoch [3/50], test_loss:0.9597\n",
            "epoch [4/50], test_loss:0.9555\n",
            "epoch [5/50], test_loss:0.9567\n",
            "epoch [6/50], test_loss:0.9550\n",
            "epoch [7/50], test_loss:0.9544\n",
            "epoch [8/50], test_loss:0.9539\n",
            "epoch [9/50], test_loss:0.9578\n",
            "epoch [10/50], test_loss:0.9562\n",
            "epoch [11/50], test_loss:0.9573\n",
            "epoch [12/50], test_loss:0.9577\n",
            "epoch [13/50], test_loss:0.9600\n",
            "epoch [14/50], test_loss:0.9626\n",
            "epoch [15/50], test_loss:0.9562\n",
            "epoch [16/50], test_loss:0.9563\n",
            "epoch [17/50], test_loss:0.9626\n",
            "epoch [18/50], test_loss:0.9564\n",
            "epoch [19/50], test_loss:0.9607\n",
            "epoch [20/50], test_loss:0.9568\n",
            "epoch [21/50], test_loss:0.9570\n",
            "epoch [22/50], test_loss:0.9605\n",
            "epoch [23/50], test_loss:0.9711\n",
            "epoch [24/50], test_loss:0.9562\n",
            "epoch [25/50], test_loss:0.9563\n",
            "epoch [26/50], test_loss:0.9587\n",
            "epoch [27/50], test_loss:0.9573\n",
            "epoch [28/50], test_loss:0.9547\n",
            "epoch [29/50], test_loss:0.9557\n",
            "epoch [30/50], test_loss:0.9561\n",
            "epoch [31/50], test_loss:0.9606\n",
            "epoch [32/50], test_loss:0.9546\n",
            "epoch [33/50], test_loss:0.9565\n",
            "epoch [34/50], test_loss:0.9533\n",
            "epoch [35/50], test_loss:0.9575\n",
            "epoch [36/50], test_loss:0.9587\n",
            "epoch [37/50], test_loss:0.9566\n",
            "epoch [38/50], test_loss:0.9570\n",
            "epoch [39/50], test_loss:0.9670\n",
            "epoch [40/50], test_loss:0.9577\n",
            "epoch [41/50], test_loss:0.9583\n",
            "epoch [42/50], test_loss:0.9618\n",
            "epoch [43/50], test_loss:0.9578\n",
            "epoch [44/50], test_loss:0.9619\n",
            "epoch [45/50], test_loss:0.9562\n",
            "epoch [46/50], test_loss:0.9589\n",
            "epoch [47/50], test_loss:0.9590\n",
            "epoch [48/50], test_loss:0.9559\n",
            "epoch [49/50], test_loss:0.9573\n",
            "epoch [50/50], test_loss:0.9658\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "min_variational_ae = 10\n",
        "for epoch in range(num_epochs):\n",
        "  with torch.no_grad():\n",
        "    loss = 0\n",
        "    for X in dataLoader_test:\n",
        "      X = X[0].to(device)\n",
        "      \n",
        "      x_reconst, mu, log_var = model(X)\n",
        "      loss += criterion(x_reconst * X[:, ind:], X[:, :ind]).item()\n",
        "    \n",
        "    print('epoch [{}/{}], test_loss:{:.4f}'.format(epoch + 1, num_epochs, loss / len(dataLoader_test)))\n",
        "    if (loss / len(dataLoader_test)) < min_variational_ae:\n",
        "      min_variational_ae = (loss / len(dataLoader_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5dKtWVcE6X1"
      },
      "source": [
        "### Perfomence of autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqfbYVzWgETA",
        "outputId": "79c9219b-32f8-495d-8f27-6f1de539265c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Undercomplete AE >> Best loss  :: 0.2046424273336143\n",
            "Regularized  AE >> Best loss  :: 0.19962961971759796\n",
            "Variational AE >> Best loss  :: 0.9533320173882601\n"
          ]
        }
      ],
      "source": [
        "print(\"Undercomplete AE >> Best loss  ::\", min_undercomplete_loss)\n",
        "print(\"Regularized  AE >> Best loss  ::\", min_reg_ae)\n",
        "print(\"Variational AE >> Best loss  ::\", min_variational_ae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs3XWqqTMlDK"
      },
      "source": [
        "####  Compare with statistical approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xqCho8H9HbA",
        "outputId": "7045b953-9693-4881-a571-800986572a5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "autoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=778, out_features=389, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=389, out_features=259, bias=True)\n",
              "    (3): Linear(in_features=259, out_features=194, bias=True)\n",
              "    (4): Tanh()\n",
              "    (5): Linear(in_features=194, out_features=50, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=194, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=194, out_features=259, bias=True)\n",
              "    (3): Tanh()\n",
              "    (4): Linear(in_features=259, out_features=389, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = autoencoder(full_data_shape , latent_size).to(device)\n",
        "model.load_state_dict(torch.load(\"minimum_loss_Regularized\"))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-NT2BUC3zdp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_data(data, model):\n",
        "  train,target =[],[]\n",
        "  for x, y in dataLoader_train:\n",
        "      x = x.to(device)\n",
        "      train.append(model(x).cpu().detach().numpy())\n",
        "      target.append(y.cpu().detach().numpy())\n",
        "  train = np.concatenate(train,  axis=0)\n",
        "  target = np.concatenate(target,  axis=0)\n",
        "  return train, target\n",
        "\n",
        "train_x, train_y = create_data(dataLoader_train, model )\n",
        "test_x, test_y = create_data(dataLoader_test, model )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1cIsOQZ-0C2",
        "outputId": "88511e1b-edb1-4d59-e46d-5a1c96bf720b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.99      0.97    106268\n",
            "         1.0       0.80      0.33      0.46      9118\n",
            "\n",
            "    accuracy                           0.94    115386\n",
            "   macro avg       0.87      0.66      0.72    115386\n",
            "weighted avg       0.93      0.94      0.93    115386\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=0).fit(train_x, train_y)\n",
        "pred = clf.predict(test_x)\n",
        "print(classification_report(test_y, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ-7AT_tgETA"
      },
      "outputs": [],
      "source": [
        "# Imputing the data with mean \n",
        "\n",
        "def create_data_mask(data, model, indicator):\n",
        "  train,target, mask =[],[], []\n",
        "  for x, y in dataLoader_train:\n",
        "      x = x.cpu().detach().numpy()\n",
        "      train.append(x[:, :indicator])\n",
        "      mask.append(x[:, indicator:])\n",
        "      target.append(y.cpu().detach().numpy())\n",
        "  train = np.concatenate(train,  axis=0)\n",
        "  mask = np.concatenate(mask,  axis=0)\n",
        "  target = np.concatenate(target,  axis=0)\n",
        "  train = np.where(mask < 1, np.ma.array(train, mask=(mask < 1)).mean(axis=0), train)\n",
        "  return train, target\n",
        "\n",
        "train_x, train_y = create_data_mask(dataLoader_train, model ,ind)\n",
        "test_x, test_y = create_data_mask(dataLoader_test, model ,ind)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp4bXk9rgETA",
        "outputId": "c0921c09-7d74-423a-d519-7762a5c98e07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.99      0.97    106268\n",
            "         1.0       0.85      0.45      0.59      9118\n",
            "\n",
            "    accuracy                           0.95    115386\n",
            "   macro avg       0.90      0.72      0.78    115386\n",
            "weighted avg       0.95      0.95      0.94    115386\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=0).fit(train_x, train_y)\n",
        "pred = clf.predict(test_x)\n",
        "print(classification_report(test_y, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3JekAd6JJ4z"
      },
      "source": [
        "## PCA Vs LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXk2mM5NttN"
      },
      "source": [
        "#### PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62E_tWHYgETA"
      },
      "outputs": [],
      "source": [
        "xTrain, yTrain = create_data(dataLoader_train, model )\n",
        "xTest, yTest = create_data(dataLoader_test, model )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c94ldExgETA"
      },
      "outputs": [],
      "source": [
        "# survice function for Explained variance ratio plotting\n",
        "def plot_explained_variance(X):\n",
        "    #Calculating Eigenvecors and eigenvalues of Covariance matrix\n",
        "    mean_vec = np.mean(X, axis=0)\n",
        "    cov_mat = np.cov(X.T)\n",
        "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "    # Create a list of (eigenvalue, eigenvector) tuples\n",
        "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
        "\n",
        "    # Sort from high to low\n",
        "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
        "\n",
        "    # Calculation of Explained Variance from the eigenvalues\n",
        "    tot = sum(eig_vals)\n",
        "    print(tot)\n",
        "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
        "    print(var_exp)\n",
        "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
        "    print(cum_var_exp)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
        "    plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
        "    plt.ylabel('Explained variance ratio')\n",
        "    plt.xlabel('Principal components')\n",
        "    plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mj0B4ubogETA",
        "outputId": "09d46dd9-1090-4f49-db17-ff34988c75e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(389.00337135865396+0j)\n",
            "[(24.94256840181095+0j), (9.651696268902132+0j), (7.5725386821467735+0j), (5.539460220077744+0j), (4.800225113920192+0j), (4.596720332111096+0j), (3.7868429404049024+0j), (3.3115658547559144+0j), (2.698839067367609+0j), (2.4078612495220386+0j), (2.0673285108727306+0j), (1.7067288852213067+0j), (1.6246544463322208+0j), (1.5814380329514557+0j), (1.541123549429841+0j), (1.4197102693185546+0j), (1.3464014003746392+0j), (1.0454948609347108+0j), (0.9147404862009205+0j), (0.8835846843080926+0j), (0.8217770557014467+0j), (0.7845909034768931+0j), (0.7353263629302932+0j), (0.6991873112781657+0j), (0.6598276609485711+0j), (0.6455064949947948+0j), (0.5725426210073029+0j), (0.5633881991808832+0j), (0.5236118928360486+0j), (0.4705751137168971+0j), (0.42769999420897464+0j), (0.41930682578336853+0j), (0.40239296325510304+0j), (0.37531779383000186+0j), (0.34594510429220676+0j), (0.3162760094110595+0j), (0.3126555089166377+0j), (0.3071885163257777+0j), (0.2956605782815557+0j), (0.2720541887725258+0j), (0.26460481241641254+0j), (0.2603748828427239+0j), (0.25376009136326805+0j), (0.23357824154348214+0j), (0.2033526228593741+0j), (0.19976774783977622+0j), (0.1894300868826225+0j), (0.17589314609686543+0j), (0.1740609264007011+0j), (0.17023387530891446+0j), (0.16130768023111644+0j), (0.15322784239134268+0j), (0.14977809573223005+0j), (0.1454772277166282+0j), (0.1374089754274784+0j), (0.1311259346570863+0j), (0.12560855438262453+0j), (0.12137066667219183+0j), (0.1182927645507673+0j), (0.11302504098355387+0j), (0.10479576854317914+0j), (0.10145888574702837+0j), (0.09761000186131179+0j), (0.09492968576173877+0j), (0.08859588263074755+0j), (0.08606276372658249+0j), (0.08370871578306477+0j), (0.08092875588120205+0j), (0.07861138164388617+0j), (0.07581938223896499+0j), (0.07444353774243616+0j), (0.07357750508515935+0j), (0.06998119753289367+0j), (0.06569082255034626+0j), (0.06483354936408844+0j), (0.06282033414109744+0j), (0.0609156463164163+0j), (0.056912906210808745+0j), (0.05549768142155678+0j), (0.05349156610875938+0j), (0.05248174094654841+0j), (0.051091480554230885+0j), (0.04883636290904184+0j), (0.047124178706989664+0j), (0.0466391132975893+0j), (0.0454405867858411+0j), (0.04149109273900254+0j), (0.040696339101326875+0j), (0.04030673228026106+0j), (0.03970129334044987+0j), (0.03896591288718343+0j), (0.03689366967676518+0j), (0.03649532385773103+0j), (0.035093993257947695+0j), (0.03464587384213598+0j), (0.03315698794527334+0j), (0.03114478178992834+0j), (0.030889037452098672+0j), (0.030028508134737873+0j), (0.028384393759713702+0j), (0.027823859034028234+0j), (0.0262453834675096+0j), (0.025616149640672024+0j), (0.024615564357375323+0j), (0.0242209059781082+0j), (0.023303315561713178+0j), (0.02275208989399726+0j), (0.02169847377597+0j), (0.021287454437843606+0j), (0.02039533563559639+0j), (0.019970849344445943+0j), (0.019584040607487238+0j), (0.018867933266822116+0j), (0.018004036994673563+0j), (0.017769676412990137+0j), (0.017511903214635888+0j), (0.01698297185985849+0j), (0.016586466424622227+0j), (0.015319273195561308+0j), (0.015079595045019482+0j), (0.014990676718189884+0j), (0.013757581548379938+0j), (0.013676069318126465+0j), (0.013112677925694661+0j), (0.012599794507987518+0j), (0.01189554902781907+0j), (0.011559309517848526+0j), (0.011282609963699588+0j), (0.010864829553061724+0j), (0.010363318233353106+0j), (0.010069230642517581+0j), (0.009645324089408288+0j), (0.009521914499517958+0j), (0.009423433941973562+0j), (0.009054033672473821+0j), (0.008464917086862983+0j), (0.008319215502940187+0j), (0.007985561228636326+0j), (0.007804642787663511+0j), (0.007347694633107736+0j), (0.007109568153967767+0j), (0.006827352578752909+0j), (0.0065625490979974175+0j), (0.006500349071654947+0j), (0.006105164259657322+0j), (0.006030658922482525+0j), (0.005835560657431792+0j), (0.005704203804683784+0j), (0.00533903048932974+0j), (0.005181621270981184+0j), (0.005049683601728831+0j), (0.004867598132886221+0j), (0.004686287131605647+0j), (0.004452501095604762+0j), (0.004267540614002572+0j), (0.0041170651781597956+0j), (0.0038972062000643477+0j), (0.0038579378368549517+0j), (0.0036189461996830813+0j), (0.003615357738781411+0j), (0.00343420264408072+0j), (0.00327873517005913+0j), (0.003011007193577094+0j), (0.0029746295947209625+0j), (0.002738372988465999+0j), (0.0026673657612424112+0j), (0.002514517789156234+0j), (0.002383424449829778+0j), (0.0023424214824520066+0j), (0.0022065348422303428+0j), (0.002124386400247651+0j), (0.002084490781193546+0j), (0.0019739325399970756+0j), (0.0018269618807473247+0j), (0.001814836229302788+0j), (0.0017653573470231539+0j), (0.0017267306158196484+0j), (0.001715558119803504+0j), (0.0015422382484940643+0j), (0.0014560073292393242+0j), (0.0013924494723496085+0j), (0.0012890000171994409+0j), (0.0012425019730282513+0j), (0.0011482709598404303+0j), (0.0011292210121724739+0j), (0.0011201496650002048+0j), (0.0010783487393371488+0j), (0.0010229602627735486+0j), (0.0010071970028978866+0j), (0.0009311891180563511+0j), (0.0009073102427731698+0j), (0.0008784749717534347+0j), (0.0008660319298745107+0j), (0.000770106371729882+0j), (0.0007380804891513327+0j), (0.0007230816539892928+0j), (0.000696518077179483+0j), (0.0006793971640798789+0j), (0.0006622610470945993+0j), (0.000638338029750627+0j), (0.0006163107117532637+0j), (0.0006085997448290768+0j), (0.0005764454179822944+0j), (0.0005574203764383786+0j), (0.0005277278441702894+0j), (0.000514256742670881+0j), (0.000498818056306779+0j), (0.0004876848193505973+0j), (0.00045884217749302966+0j), (0.0004359225109505715+0j), (0.00042690157525459454+0j), (0.00040399678594400904+0j), (0.0003905180404571029+0j), (0.00035524978404286294+0j), (0.0003529938952436072+0j), (0.00034513637029485593+0j), (0.0003290533343352638+0j), (0.0003139791294501393+0j), (0.00029309180030241276+0j), (0.00028651476795519203+0j), (0.00027837231657079175+0j), (0.0002675559862646069+0j), (0.0002625416650213381+0j), (0.00024723715513015+0j), (0.00024083320894855657+0j), (0.00023349418711369726+0j), (0.0002245421775882803+0j), (0.0002120959557526097+0j), (0.0002094158797567087+0j), (0.00019782952606830782+0j), (0.00019469596171487773+0j), (0.00018974009219323128+0j), (0.0001791021159621981+0j), (0.00017014087379137904+0j), (0.00016092278487822893+0j), (0.00015516901607918497+0j), (0.00014987375891932446+0j), (0.0001395409734739575+0j), (0.00013745831037614134+0j), (0.00013219403438774927+0j), (0.0001273327654128255+0j), (0.00011401011129964077+0j), (0.0001114719819518134+0j), (0.00010496167026512384+0j), (0.00010312378922829431+0j), (9.770875658041006e-05+0j), (9.320983443554859e-05+0j), (8.799178040197731e-05+0j), (8.308407523616035e-05+0j), (7.764686803279192e-05+0j), (7.108744397284572e-05+0j), (6.867545216405554e-05+0j), (6.474926778214613e-05+0j), (6.288556602900094e-05+0j), (5.826249427009473e-05+0j), (5.558766265984127e-05+0j), (4.764388834993194e-05+0j), (4.078231657405998e-05+0j), (3.817424583053048e-05+0j), (7.3737413116098e-14+0j), (7.221660640064588e-14+0j), (6.964656120968605e-14+0j), (6.697507550792052e-14+0j), (6.432268276666946e-14+0j), (6.370218027259742e-14+0j), (6.272792840116273e-14+0j), (6.193560007630516e-14+0j), (6.079660053134034e-14+0j), (5.899600677129658e-14+0j), (5.729611993809425e-14+0j), (5.452511490451539e-14+0j), (5.422160696343094e-14+0j), (5.3958234328314437e-14+0j), (5.2880855892561015e-14+0j), (5.238428704559999e-14+0j), (5.1201081479352856e-14+0j), (5.0645132226277774e-14+0j), (4.9733543212684416e-14+0j), (4.9004496674431094e-14+0j), (4.8708585889049646e-14+0j), (4.827426488281821e-14+0j), (4.793022359627439e-14+0j), (4.7251343139627085e-14+0j), (4.6619490143930996e-14+0j), (4.638206019368543e-14+0j), (4.606058375833447e-14+0j), (4.4859840038170696e-14+2.2445855234760624e-17j), (4.4859840038170696e-14-2.2445855234760624e-17j), (4.427064606981206e-14+0j), (4.39323033216473e-14+0j), (4.3432462664690945e-14+0j), (4.329576667044458e-14+0j), (4.271588289901122e-14+0j), (4.2353506553791283e-14+0j), (4.203417687484372e-14+0j), (4.167610992898029e-14+0j), (4.110169183330251e-14+0j), (4.0640928246701204e-14+0j), (4.0438293153298936e-14+0j), (4.0117877639674e-14+0j), (4.005480506126404e-14+0j), (3.9942631046870364e-14+0j), (3.887713054991202e-14+0j), (3.881298445826991e-14+0j), (3.8513140731598496e-14+0j), (3.824285240897439e-14+0j), (3.773811032268586e-14+0j), (3.750326375906475e-14+0j), (3.7085643445203823e-14+0j), (3.659459299464985e-14+0j), (3.6492876509447885e-14+0j), (3.628360006029126e-14+0j), (3.5688342702542665e-14+0j), (3.563589153841474e-14+0j), (3.5321369393415524e-14+0j), (3.482572358418552e-14+0j), (3.466274231072156e-14+0j), (3.438702308546008e-14+0j), (3.4145715094062544e-14+0j), (3.359832950928723e-14+0j), (3.3510496400463686e-14+0j), (3.3107085637857944e-14+0j), (3.296079064179828e-14+0j), (3.2584659325788667e-14+0j), (3.2498511261064464e-14+0j), (3.2243724329823e-14+0j), (3.2041894843825396e-14+0j), (3.1801371526938524e-14+0j), (3.146917005771917e-14+0j), (3.118521030603475e-14+0j), (3.0713039906036106e-14+0j), (3.0631686485075276e-14+0j), (3.0247456700099804e-14+0j), (3.005624612285027e-14+0j), (2.9861099558025547e-14+0j), (2.953565306244097e-14+0j), (2.9250086851000475e-14+0j), (2.9173462808543854e-14+0j), (2.9018620716157245e-14+0j), (2.876435025900889e-14+0j), (2.845344289610356e-14+0j), (2.8150268851921737e-14+0j), (2.778834782970987e-14+0j), (2.7555383987636085e-14+0j), (2.724177354452888e-14+0j), (2.7012869340659432e-14+0j), (2.696725669206223e-14+0j), (2.6618906933105484e-14+0j), (2.651815253159197e-14+0j), (2.620885210769027e-14+0j), (2.605013799265641e-14+0j), (2.570899985054444e-14+0j), (2.5408424797653876e-14+0j), (2.532476538501674e-14+0j), (2.501668670805727e-14+0j), (2.492888486093326e-14+0j), (2.4595905571070056e-14+0j), (2.4428439909532705e-14+0j), (2.4245212477399957e-14+0j), (2.3899048588753408e-14+0j), (2.3520897460962584e-14+0j), (2.3383946864637757e-14+0j), (2.2968876569395027e-14+0j), (2.2924665703933217e-14+0j), (2.2732456972581487e-14+0j), (2.2322369940652917e-14+0j), (2.213668026614971e-14+0j), (2.189808533930836e-14+4.3789163141067336e-17j), (2.189808533930836e-14-4.3789163141067336e-17j), (2.1674433358558973e-14+0j), (2.1517101075132745e-14+0j), (2.112618234779016e-14+0j), (2.060993115110199e-14+0j), (2.0477781607596573e-14+0j), (2.028742717919269e-14+0j), (1.9981545251388075e-14+0j), (1.9890719827115696e-14+0j), (1.955775792355988e-14+0j), (1.918916531653558e-14+0j), (1.9024844440286523e-14+0j), (1.8763849498115198e-14+0j), (1.8040342756618762e-14+0j), (1.7795770851464864e-14+0j), (1.763021192106329e-14+0j), (1.7082874617347444e-14+0j), (1.653883585344917e-14+0j), (1.5990560952429542e-14+0j), (1.50320628108898e-14+0j), (1.4619584882822734e-14+0j)]\n",
            "[ 24.9425684 +0.00000000e+00j  34.59426467+0.00000000e+00j\n",
            "  42.16680335+0.00000000e+00j  47.70626357+0.00000000e+00j\n",
            "  52.50648869+0.00000000e+00j  57.10320902+0.00000000e+00j\n",
            "  60.89005196+0.00000000e+00j  64.20161781+0.00000000e+00j\n",
            "  66.90045688+0.00000000e+00j  69.30831813+0.00000000e+00j\n",
            "  71.37564664+0.00000000e+00j  73.08237553+0.00000000e+00j\n",
            "  74.70702997+0.00000000e+00j  76.28846801+0.00000000e+00j\n",
            "  77.82959156+0.00000000e+00j  79.24930183+0.00000000e+00j\n",
            "  80.59570323+0.00000000e+00j  81.64119809+0.00000000e+00j\n",
            "  82.55593857+0.00000000e+00j  83.43952326+0.00000000e+00j\n",
            "  84.26130031+0.00000000e+00j  85.04589122+0.00000000e+00j\n",
            "  85.78121758+0.00000000e+00j  86.48040489+0.00000000e+00j\n",
            "  87.14023255+0.00000000e+00j  87.78573905+0.00000000e+00j\n",
            "  88.35828167+0.00000000e+00j  88.92166987+0.00000000e+00j\n",
            "  89.44528176+0.00000000e+00j  89.91585687+0.00000000e+00j\n",
            "  90.34355687+0.00000000e+00j  90.76286369+0.00000000e+00j\n",
            "  91.16525666+0.00000000e+00j  91.54057445+0.00000000e+00j\n",
            "  91.88651955+0.00000000e+00j  92.20279556+0.00000000e+00j\n",
            "  92.51545107+0.00000000e+00j  92.82263959+0.00000000e+00j\n",
            "  93.11830017+0.00000000e+00j  93.39035436+0.00000000e+00j\n",
            "  93.65495917+0.00000000e+00j  93.91533405+0.00000000e+00j\n",
            "  94.16909414+0.00000000e+00j  94.40267238+0.00000000e+00j\n",
            "  94.60602501+0.00000000e+00j  94.80579275+0.00000000e+00j\n",
            "  94.99522284+0.00000000e+00j  95.17111599+0.00000000e+00j\n",
            "  95.34517691+0.00000000e+00j  95.51541079+0.00000000e+00j\n",
            "  95.67671847+0.00000000e+00j  95.82994631+0.00000000e+00j\n",
            "  95.97972441+0.00000000e+00j  96.12520164+0.00000000e+00j\n",
            "  96.26261061+0.00000000e+00j  96.39373655+0.00000000e+00j\n",
            "  96.5193451 +0.00000000e+00j  96.64071577+0.00000000e+00j\n",
            "  96.75900853+0.00000000e+00j  96.87203357+0.00000000e+00j\n",
            "  96.97682934+0.00000000e+00j  97.07828823+0.00000000e+00j\n",
            "  97.17589823+0.00000000e+00j  97.27082791+0.00000000e+00j\n",
            "  97.3594238 +0.00000000e+00j  97.44548656+0.00000000e+00j\n",
            "  97.52919528+0.00000000e+00j  97.61012403+0.00000000e+00j\n",
            "  97.68873541+0.00000000e+00j  97.7645548 +0.00000000e+00j\n",
            "  97.83899833+0.00000000e+00j  97.91257584+0.00000000e+00j\n",
            "  97.98255704+0.00000000e+00j  98.04824786+0.00000000e+00j\n",
            "  98.11308141+0.00000000e+00j  98.17590174+0.00000000e+00j\n",
            "  98.23681739+0.00000000e+00j  98.2937303 +0.00000000e+00j\n",
            "  98.34922798+0.00000000e+00j  98.40271954+0.00000000e+00j\n",
            "  98.45520128+0.00000000e+00j  98.50629276+0.00000000e+00j\n",
            "  98.55512913+0.00000000e+00j  98.60225331+0.00000000e+00j\n",
            "  98.64889242+0.00000000e+00j  98.69433301+0.00000000e+00j\n",
            "  98.7358241 +0.00000000e+00j  98.77652044+0.00000000e+00j\n",
            "  98.81682717+0.00000000e+00j  98.85652846+0.00000000e+00j\n",
            "  98.89549438+0.00000000e+00j  98.93238805+0.00000000e+00j\n",
            "  98.96888337+0.00000000e+00j  99.00397736+0.00000000e+00j\n",
            "  99.03862324+0.00000000e+00j  99.07178022+0.00000000e+00j\n",
            "  99.10292501+0.00000000e+00j  99.13381404+0.00000000e+00j\n",
            "  99.16384255+0.00000000e+00j  99.19222695+0.00000000e+00j\n",
            "  99.22005081+0.00000000e+00j  99.24629619+0.00000000e+00j\n",
            "  99.27191234+0.00000000e+00j  99.2965279 +0.00000000e+00j\n",
            "  99.32074881+0.00000000e+00j  99.34405212+0.00000000e+00j\n",
            "  99.36680421+0.00000000e+00j  99.38850269+0.00000000e+00j\n",
            "  99.40979014+0.00000000e+00j  99.43018548+0.00000000e+00j\n",
            "  99.45015633+0.00000000e+00j  99.46974037+0.00000000e+00j\n",
            "  99.4886083 +0.00000000e+00j  99.50661234+0.00000000e+00j\n",
            "  99.52438201+0.00000000e+00j  99.54189392+0.00000000e+00j\n",
            "  99.55887689+0.00000000e+00j  99.57546336+0.00000000e+00j\n",
            "  99.59078263+0.00000000e+00j  99.60586222+0.00000000e+00j\n",
            "  99.6208529 +0.00000000e+00j  99.63461048+0.00000000e+00j\n",
            "  99.64828655+0.00000000e+00j  99.66139923+0.00000000e+00j\n",
            "  99.67399902+0.00000000e+00j  99.68589457+0.00000000e+00j\n",
            "  99.69745388+0.00000000e+00j  99.70873649+0.00000000e+00j\n",
            "  99.71960132+0.00000000e+00j  99.72996464+0.00000000e+00j\n",
            "  99.74003387+0.00000000e+00j  99.7496792 +0.00000000e+00j\n",
            "  99.75920111+0.00000000e+00j  99.76862454+0.00000000e+00j\n",
            "  99.77767858+0.00000000e+00j  99.78614349+0.00000000e+00j\n",
            "  99.79446271+0.00000000e+00j  99.80244827+0.00000000e+00j\n",
            "  99.81025291+0.00000000e+00j  99.81760061+0.00000000e+00j\n",
            "  99.82471018+0.00000000e+00j  99.83153753+0.00000000e+00j\n",
            "  99.83810008+0.00000000e+00j  99.84460043+0.00000000e+00j\n",
            "  99.85070559+0.00000000e+00j  99.85673625+0.00000000e+00j\n",
            "  99.86257181+0.00000000e+00j  99.86827602+0.00000000e+00j\n",
            "  99.87361505+0.00000000e+00j  99.87879667+0.00000000e+00j\n",
            "  99.88384635+0.00000000e+00j  99.88871395+0.00000000e+00j\n",
            "  99.89340024+0.00000000e+00j  99.89785274+0.00000000e+00j\n",
            "  99.90212028+0.00000000e+00j  99.90623734+0.00000000e+00j\n",
            "  99.91013455+0.00000000e+00j  99.91399249+0.00000000e+00j\n",
            "  99.91761143+0.00000000e+00j  99.92122679+0.00000000e+00j\n",
            "  99.92466099+0.00000000e+00j  99.92793973+0.00000000e+00j\n",
            "  99.93095074+0.00000000e+00j  99.93392537+0.00000000e+00j\n",
            "  99.93666374+0.00000000e+00j  99.9393311 +0.00000000e+00j\n",
            "  99.94184562+0.00000000e+00j  99.94422905+0.00000000e+00j\n",
            "  99.94657147+0.00000000e+00j  99.948778  +0.00000000e+00j\n",
            "  99.95090239+0.00000000e+00j  99.95298688+0.00000000e+00j\n",
            "  99.95496081+0.00000000e+00j  99.95678777+0.00000000e+00j\n",
            "  99.95860261+0.00000000e+00j  99.96036797+0.00000000e+00j\n",
            "  99.9620947 +0.00000000e+00j  99.96381026+0.00000000e+00j\n",
            "  99.96535249+0.00000000e+00j  99.9668085 +0.00000000e+00j\n",
            "  99.96820095+0.00000000e+00j  99.96948995+0.00000000e+00j\n",
            "  99.97073245+0.00000000e+00j  99.97188072+0.00000000e+00j\n",
            "  99.97300995+0.00000000e+00j  99.9741301 +0.00000000e+00j\n",
            "  99.97520844+0.00000000e+00j  99.9762314 +0.00000000e+00j\n",
            "  99.9772386 +0.00000000e+00j  99.97816979+0.00000000e+00j\n",
            "  99.9790771 +0.00000000e+00j  99.97995558+0.00000000e+00j\n",
            "  99.98082161+0.00000000e+00j  99.98159171+0.00000000e+00j\n",
            "  99.98232979+0.00000000e+00j  99.98305288+0.00000000e+00j\n",
            "  99.98374939+0.00000000e+00j  99.98442879+0.00000000e+00j\n",
            "  99.98509105+0.00000000e+00j  99.98572939+0.00000000e+00j\n",
            "  99.9863457 +0.00000000e+00j  99.9869543 +0.00000000e+00j\n",
            "  99.98753075+0.00000000e+00j  99.98808817+0.00000000e+00j\n",
            "  99.98861589+0.00000000e+00j  99.98913015+0.00000000e+00j\n",
            "  99.98962897+0.00000000e+00j  99.99011665+0.00000000e+00j\n",
            "  99.9905755 +0.00000000e+00j  99.99101142+0.00000000e+00j\n",
            "  99.99143832+0.00000000e+00j  99.99184232+0.00000000e+00j\n",
            "  99.99223283+0.00000000e+00j  99.99258808+0.00000000e+00j\n",
            "  99.99294108+0.00000000e+00j  99.99328622+0.00000000e+00j\n",
            "  99.99361527+0.00000000e+00j  99.99392925+0.00000000e+00j\n",
            "  99.99422234+0.00000000e+00j  99.99450885+0.00000000e+00j\n",
            "  99.99478723+0.00000000e+00j  99.99505478+0.00000000e+00j\n",
            "  99.99531732+0.00000000e+00j  99.99556456+0.00000000e+00j\n",
            "  99.99580539+0.00000000e+00j  99.99603889+0.00000000e+00j\n",
            "  99.99626343+0.00000000e+00j  99.99647553+0.00000000e+00j\n",
            "  99.99668494+0.00000000e+00j  99.99688277+0.00000000e+00j\n",
            "  99.99707747+0.00000000e+00j  99.99726721+0.00000000e+00j\n",
            "  99.99744631+0.00000000e+00j  99.99761645+0.00000000e+00j\n",
            "  99.99777737+0.00000000e+00j  99.99793254+0.00000000e+00j\n",
            "  99.99808242+0.00000000e+00j  99.99822196+0.00000000e+00j\n",
            "  99.99835942+0.00000000e+00j  99.99849161+0.00000000e+00j\n",
            "  99.99861894+0.00000000e+00j  99.99873295+0.00000000e+00j\n",
            "  99.99884442+0.00000000e+00j  99.99894939+0.00000000e+00j\n",
            "  99.99905251+0.00000000e+00j  99.99915022+0.00000000e+00j\n",
            "  99.99924343+0.00000000e+00j  99.99933142+0.00000000e+00j\n",
            "  99.9994145 +0.00000000e+00j  99.99949215+0.00000000e+00j\n",
            "  99.99956324+0.00000000e+00j  99.99963191+0.00000000e+00j\n",
            "  99.99969666+0.00000000e+00j  99.99975955+0.00000000e+00j\n",
            "  99.99981781+0.00000000e+00j  99.9998734 +0.00000000e+00j\n",
            "  99.99992104+0.00000000e+00j  99.99996183+0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +2.24458552e-17j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +4.37891631e-17j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j 100.        +0.00000000e+00j\n",
            " 100.        +0.00000000e+00j]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py:789: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  points = np.array(args, dtype=float).reshape(2, 2)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1317: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  return np.asarray(x, float)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py:789: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  points = np.array(args, dtype=float).reshape(2, 2)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_explained_variance(sc.fit_transform(xTrain))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPLwqfGBgETB"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components= 150)\n",
        "xTrain_reduced = pca.fit(xTrain).transform(xTrain)\n",
        "xTest_reduced = pca.fit(xTest).transform(xTest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbj1mXnwMzoS",
        "outputId": "1a4b990d-c2b6-4e7d-a29f-b5510f05f727"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.99      0.97    106268\n",
            "         1.0       0.80      0.32      0.46      9118\n",
            "\n",
            "    accuracy                           0.94    115386\n",
            "   macro avg       0.87      0.66      0.71    115386\n",
            "weighted avg       0.93      0.94      0.93    115386\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=0).fit(xTrain_reduced, yTrain)\n",
        "pred = clf.predict(xTest_reduced)\n",
        "print(classification_report(yTest, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfyxC7JDNzq1"
      },
      "source": [
        "#### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j442O1FMzrb"
      },
      "outputs": [],
      "source": [
        "lda = LDA()\n",
        "xTrain_lda = lda.fit(xTrain, yTrain).transform(xTrain)\n",
        "xTest_lda = lda.transform(xTest)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaNESb5IMzuM",
        "outputId": "df22c940-f9d0-41a2-a023-09fc2f397526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.99      0.97    106268\n",
            "         1.0       0.79      0.35      0.48      9118\n",
            "\n",
            "    accuracy                           0.94    115386\n",
            "   macro avg       0.87      0.67      0.73    115386\n",
            "weighted avg       0.93      0.94      0.93    115386\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=0).fit(xTrain_lda, yTrain)\n",
        "pred = clf.predict(xTest_lda)\n",
        "print(classification_report(yTest, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcHZHt9Pad6"
      },
      "source": [
        "**observation**\n",
        "\n",
        "From classification report, we can find that PCA is better than LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bE1Jo3ZZc4i"
      },
      "source": [
        "# Task 2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ir32WNh5Fcb"
      },
      "source": [
        "**Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6H2FZsp_KcT"
      },
      "outputs": [],
      "source": [
        "#! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
        "#clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTMhjVly_M7B"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install interpret\n"
      ],
      "metadata": {
        "id": "6R_LWqkMakoA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OTQvatGWZetZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from sklearn.preprocessing import  RobustScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "gw75y4TFZiFP",
        "outputId": "2a9ee0b3-0a38-4926-e2a9-f054343a5fea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id       dur proto service state  spkts  dpkts  sbytes  dbytes  \\\n",
              "0   1  0.000011   udp       -   INT      2      0     496       0   \n",
              "1   2  0.000008   udp       -   INT      2      0    1762       0   \n",
              "2   3  0.000005   udp       -   INT      2      0    1068       0   \n",
              "3   4  0.000006   udp       -   INT      2      0     900       0   \n",
              "4   5  0.000010   udp       -   INT      2      0    2126       0   \n",
              "\n",
              "          rate  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
              "0   90909.0902  ...                 1               2             0   \n",
              "1  125000.0003  ...                 1               2             0   \n",
              "2  200000.0051  ...                 1               3             0   \n",
              "3  166666.6608  ...                 1               3             0   \n",
              "4  100000.0025  ...                 1               3             0   \n",
              "\n",
              "   ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \\\n",
              "0           0                 0           1           2                0   \n",
              "1           0                 0           1           2                0   \n",
              "2           0                 0           1           3                0   \n",
              "3           0                 0           2           3                0   \n",
              "4           0                 0           2           3                0   \n",
              "\n",
              "   attack_cat  label  \n",
              "0      Normal      0  \n",
              "1      Normal      0  \n",
              "2      Normal      0  \n",
              "3      Normal      0  \n",
              "4      Normal      0  \n",
              "\n",
              "[5 rows x 45 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5d28ea0-a81f-4ede-b01e-80ffa898a9d3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>dur</th>\n",
              "      <th>proto</th>\n",
              "      <th>service</th>\n",
              "      <th>state</th>\n",
              "      <th>spkts</th>\n",
              "      <th>dpkts</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>...</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>is_ftp_login</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "      <th>attack_cat</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>496</td>\n",
              "      <td>0</td>\n",
              "      <td>90909.0902</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1762</td>\n",
              "      <td>0</td>\n",
              "      <td>125000.0003</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1068</td>\n",
              "      <td>0</td>\n",
              "      <td>200000.0051</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>900</td>\n",
              "      <td>0</td>\n",
              "      <td>166666.6608</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>udp</td>\n",
              "      <td>-</td>\n",
              "      <td>INT</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2126</td>\n",
              "      <td>0</td>\n",
              "      <td>100000.0025</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 45 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5d28ea0-a81f-4ede-b01e-80ffa898a9d3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5d28ea0-a81f-4ede-b01e-80ffa898a9d3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5d28ea0-a81f-4ede-b01e-80ffa898a9d3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Getting Data\n",
        "df_train = pd.read_csv('UNSW_NB15_training-set.csv')\n",
        "df_test = pd.read_csv('UNSW_NB15_testing-set.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36wQ9XfkZiHz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz5qVfs77sf0"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBcPBWkwZiKg",
        "outputId": "5c28fad2-7263-4af6-b04e-5945d47e71a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ct_src_dport_ltm', 'ct_srv_dst', 'ct_srv_src', 'dloss', 'dpkts', 'is_ftp_login', 'sinpkt', 'sloss', 'spkts', 'swin', 'tcprtt']\n"
          ]
        }
      ],
      "source": [
        "## Find some useless features and remove it \n",
        "# Refernce : https://github.com/ydataai/pandas-profiling/issues/190\n",
        "import pandas_profiling\n",
        "\n",
        "pd_prof = df_train.profile_report()\n",
        "rejected = pd_prof.get_rejected_variables()\n",
        "print(rejected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yad_2wImZiNZ"
      },
      "outputs": [],
      "source": [
        "## Clear the train data \n",
        "df_train.drop(rejected, axis=1, inplace=True)\n",
        "y_train  = df_train['attack_cat']\n",
        "df_train.drop([\"label\",\"attack_cat\", \"id\"], axis=1, inplace=True)\n",
        "\n",
        "df_test.drop(rejected, axis=1, inplace=True)\n",
        "y_test  = df_test['attack_cat']\n",
        "df_test.drop([\"label\",\"attack_cat\", \"id\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"data train shape: \", df_train.shape)\n",
        "print(\"data test shape: \", df_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jFVm1MnbSTk",
        "outputId": "820dc276-240f-4c1d-cbbe-73a2f484b1aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data train shape:  (82332, 31)\n",
            "data test shape:  (175341, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "N8GMl1N_b-uT",
        "outputId": "d82e30c9-89d3-445e-a2f8-732b5b7691f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                dur        sbytes        dbytes          rate          sttl  \\\n",
              "count  82332.000000  8.233200e+04  8.233200e+04  8.233200e+04  82332.000000   \n",
              "mean       1.006756  7.993908e+03  1.323379e+04  8.241089e+04    180.967667   \n",
              "std        4.710444  1.716423e+05  1.514715e+05  1.486204e+05    101.513358   \n",
              "min        0.000000  2.400000e+01  0.000000e+00  0.000000e+00      0.000000   \n",
              "25%        0.000008  1.140000e+02  0.000000e+00  2.860611e+01     62.000000   \n",
              "50%        0.014138  5.340000e+02  1.780000e+02  2.650177e+03    254.000000   \n",
              "75%        0.719360  1.280000e+03  9.560000e+02  1.111111e+05    254.000000   \n",
              "max       59.999989  1.435577e+07  1.465753e+07  1.000000e+06    255.000000   \n",
              "\n",
              "               dttl         sload         dload        dinpkt          sjit  \\\n",
              "count  82332.000000  8.233200e+04  8.233200e+04  82332.000000  8.233200e+04   \n",
              "mean      95.713003  6.454902e+07  6.305470e+05    121.701284  6.363075e+03   \n",
              "std      116.667722  1.798618e+08  2.393001e+06   1292.378499  5.672402e+04   \n",
              "min        0.000000  0.000000e+00  0.000000e+00      0.000000  0.000000e+00   \n",
              "25%        0.000000  1.120247e+04  0.000000e+00      0.000000  0.000000e+00   \n",
              "50%       29.000000  5.770032e+05  2.112951e+03      0.010000  1.762392e+01   \n",
              "75%      252.000000  6.514286e+07  1.585808e+04     63.136369  3.219332e+03   \n",
              "max      253.000000  5.268000e+09  2.082111e+07  57739.240000  1.483831e+06   \n",
              "\n",
              "       ...   trans_depth  response_body_len  ct_state_ttl    ct_dst_ltm  \\\n",
              "count  ...  82332.000000       8.233200e+04  82332.000000  82332.000000   \n",
              "mean   ...      0.094277       1.595372e+03      1.369273      5.744923   \n",
              "std    ...      0.542922       3.806697e+04      1.067188      8.418112   \n",
              "min    ...      0.000000       0.000000e+00      0.000000      1.000000   \n",
              "25%    ...      0.000000       0.000000e+00      1.000000      1.000000   \n",
              "50%    ...      0.000000       0.000000e+00      1.000000      2.000000   \n",
              "75%    ...      0.000000       0.000000e+00      2.000000      6.000000   \n",
              "max    ...    131.000000       5.242880e+06      6.000000     59.000000   \n",
              "\n",
              "       ct_dst_sport_ltm  ct_dst_src_ltm    ct_ftp_cmd  ct_flw_http_mthd  \\\n",
              "count      82332.000000    82332.000000  82332.000000      82332.000000   \n",
              "mean           3.663011        7.456360      0.008381          0.129743   \n",
              "std            5.915386       11.415191      0.092485          0.638683   \n",
              "min            1.000000        1.000000      0.000000          0.000000   \n",
              "25%            1.000000        1.000000      0.000000          0.000000   \n",
              "50%            1.000000        3.000000      0.000000          0.000000   \n",
              "75%            3.000000        6.000000      0.000000          0.000000   \n",
              "max           38.000000       63.000000      2.000000         16.000000   \n",
              "\n",
              "         ct_src_ltm  is_sm_ips_ports  \n",
              "count  82332.000000     82332.000000  \n",
              "mean       6.468360         0.011126  \n",
              "std        8.543927         0.104891  \n",
              "min        1.000000         0.000000  \n",
              "25%        1.000000         0.000000  \n",
              "50%        3.000000         0.000000  \n",
              "75%        7.000000         0.000000  \n",
              "max       60.000000         1.000000  \n",
              "\n",
              "[8 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e1ab9e2-3888-4f66-8def-43720dcc7040\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dur</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sttl</th>\n",
              "      <th>dttl</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>dinpkt</th>\n",
              "      <th>sjit</th>\n",
              "      <th>...</th>\n",
              "      <th>trans_depth</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>ct_state_ttl</th>\n",
              "      <th>ct_dst_ltm</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>82332.000000</td>\n",
              "      <td>8.233200e+04</td>\n",
              "      <td>8.233200e+04</td>\n",
              "      <td>8.233200e+04</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>8.233200e+04</td>\n",
              "      <td>8.233200e+04</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>8.233200e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>8.233200e+04</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "      <td>82332.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.006756</td>\n",
              "      <td>7.993908e+03</td>\n",
              "      <td>1.323379e+04</td>\n",
              "      <td>8.241089e+04</td>\n",
              "      <td>180.967667</td>\n",
              "      <td>95.713003</td>\n",
              "      <td>6.454902e+07</td>\n",
              "      <td>6.305470e+05</td>\n",
              "      <td>121.701284</td>\n",
              "      <td>6.363075e+03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.094277</td>\n",
              "      <td>1.595372e+03</td>\n",
              "      <td>1.369273</td>\n",
              "      <td>5.744923</td>\n",
              "      <td>3.663011</td>\n",
              "      <td>7.456360</td>\n",
              "      <td>0.008381</td>\n",
              "      <td>0.129743</td>\n",
              "      <td>6.468360</td>\n",
              "      <td>0.011126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.710444</td>\n",
              "      <td>1.716423e+05</td>\n",
              "      <td>1.514715e+05</td>\n",
              "      <td>1.486204e+05</td>\n",
              "      <td>101.513358</td>\n",
              "      <td>116.667722</td>\n",
              "      <td>1.798618e+08</td>\n",
              "      <td>2.393001e+06</td>\n",
              "      <td>1292.378499</td>\n",
              "      <td>5.672402e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>0.542922</td>\n",
              "      <td>3.806697e+04</td>\n",
              "      <td>1.067188</td>\n",
              "      <td>8.418112</td>\n",
              "      <td>5.915386</td>\n",
              "      <td>11.415191</td>\n",
              "      <td>0.092485</td>\n",
              "      <td>0.638683</td>\n",
              "      <td>8.543927</td>\n",
              "      <td>0.104891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.400000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>1.140000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.860611e+01</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.120247e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.014138</td>\n",
              "      <td>5.340000e+02</td>\n",
              "      <td>1.780000e+02</td>\n",
              "      <td>2.650177e+03</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>5.770032e+05</td>\n",
              "      <td>2.112951e+03</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>1.762392e+01</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.719360</td>\n",
              "      <td>1.280000e+03</td>\n",
              "      <td>9.560000e+02</td>\n",
              "      <td>1.111111e+05</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>252.000000</td>\n",
              "      <td>6.514286e+07</td>\n",
              "      <td>1.585808e+04</td>\n",
              "      <td>63.136369</td>\n",
              "      <td>3.219332e+03</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>59.999989</td>\n",
              "      <td>1.435577e+07</td>\n",
              "      <td>1.465753e+07</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>255.000000</td>\n",
              "      <td>253.000000</td>\n",
              "      <td>5.268000e+09</td>\n",
              "      <td>2.082111e+07</td>\n",
              "      <td>57739.240000</td>\n",
              "      <td>1.483831e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>5.242880e+06</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 28 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e1ab9e2-3888-4f66-8def-43720dcc7040')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e1ab9e2-3888-4f66-8def-43720dcc7040 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e1ab9e2-3888-4f66-8def-43720dcc7040');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqPx5zR_ZiSH",
        "outputId": "cc02d4bb-880e-4748-a5a2-16cfcf589c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['proto', 'service', 'state']\n"
          ]
        }
      ],
      "source": [
        "# Find categorical data\n",
        "list_cat = list(df_train.select_dtypes('object').columns.values)\n",
        "print(list_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wgrwanHDZiU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cc54f4-74f0-4b3d-b6ed-389833d7cd6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(82332, 31)\n"
          ]
        }
      ],
      "source": [
        "# Convert the categorical data\n",
        "dataset = [df_train, df_test]\n",
        "for df in dataset:\n",
        "  for col in list_cat:\n",
        "      if col in df.columns:\n",
        "          le = LabelEncoder()\n",
        "          le.fit(list(df[col].astype(str).values))\n",
        "          df[col] = le.transform(list(df[col].astype(str).values))\n",
        "\n",
        "print(df_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "hZCCCeNMZiYb",
        "outputId": "77422891-ab9e-4c31-9604-ac1827adea50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        dur     proto  service     state    sbytes    dbytes      rate  sttl  \\\n",
              "0 -0.000236  0.047619      0.0  0.333333 -0.000051 -0.000163  0.088259   0.0   \n",
              "1 -0.000237  0.047619      0.0  0.333333  0.001659 -0.000163  0.122350   0.0   \n",
              "2 -0.000237  0.047619      0.0  0.333333  0.000721 -0.000163  0.197350   0.0   \n",
              "3 -0.000237  0.047619      0.0  0.333333  0.000494 -0.000163  0.164016   0.0   \n",
              "4 -0.000236  0.047619      0.0  0.333333  0.002150 -0.000163  0.097350   0.0   \n",
              "\n",
              "       dttl     sload  ...  trans_depth  response_body_len  ct_state_ttl  \\\n",
              "0 -0.115079  0.084486  ...          0.0                0.0      0.166667   \n",
              "1 -0.115079  0.413733  ...          0.0                0.0      0.166667   \n",
              "2 -0.115079  0.401233  ...          0.0                0.0      0.166667   \n",
              "3 -0.115079  0.281684  ...          0.0                0.0      0.166667   \n",
              "4 -0.115079  0.399353  ...          0.0                0.0      0.166667   \n",
              "\n",
              "   ct_dst_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  ct_ftp_cmd  ct_flw_http_mthd  \\\n",
              "0   -0.023256               0.0       -0.018868         0.0               0.0   \n",
              "1   -0.023256               0.0       -0.018868         0.0               0.0   \n",
              "2   -0.023256               0.0        0.000000         0.0               0.0   \n",
              "3    0.000000               0.0        0.000000         0.0               0.0   \n",
              "4    0.000000               0.0        0.000000         0.0               0.0   \n",
              "\n",
              "   ct_src_ltm  is_sm_ips_ports  \n",
              "0   -0.045455              0.0  \n",
              "1   -0.045455              0.0  \n",
              "2   -0.045455              0.0  \n",
              "3   -0.022727              0.0  \n",
              "4   -0.022727              0.0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-def6ef2c-7966-4e9a-bead-103816995084\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dur</th>\n",
              "      <th>proto</th>\n",
              "      <th>service</th>\n",
              "      <th>state</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sttl</th>\n",
              "      <th>dttl</th>\n",
              "      <th>sload</th>\n",
              "      <th>...</th>\n",
              "      <th>trans_depth</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>ct_state_ttl</th>\n",
              "      <th>ct_dst_ltm</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.000236</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>-0.000051</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0.088259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.115079</td>\n",
              "      <td>0.084486</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>-0.023256</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.018868</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045455</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.000237</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.001659</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0.122350</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.115079</td>\n",
              "      <td>0.413733</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>-0.023256</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.018868</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045455</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.000237</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0.197350</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.115079</td>\n",
              "      <td>0.401233</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>-0.023256</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.045455</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.000237</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0.164016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.115079</td>\n",
              "      <td>0.281684</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.022727</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.000236</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.002150</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0.097350</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.115079</td>\n",
              "      <td>0.399353</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.022727</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-def6ef2c-7966-4e9a-bead-103816995084')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-def6ef2c-7966-4e9a-bead-103816995084 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-def6ef2c-7966-4e9a-bead-103816995084');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "## Scale the data using Robust Scaler\n",
        "scaler = RobustScaler(quantile_range=(0.1, 99.9 ))\n",
        "df_train_sc = pd.DataFrame(scaler.fit_transform(df_train[list(df_train.columns)].values),columns=list(df_train.columns))\n",
        "df_test_sc = pd.DataFrame(scaler.transform(df_test[list(df_test.columns)].values),columns=list(df_test.columns))\n",
        "\n",
        "df_train_sc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XvKmKHJdSqiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75bb4e53-c22c-42d4-d2f3-4c7cfe6b6099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test = le.transform(y_test.values.reshape(-1, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj2W_Hh9Oz3c"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CTagUaoxVSie"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6MVRhCmZibR",
        "outputId": "cbec6e28-e7c3-49fc-bed5-1346a8e4e212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "batch=128\n",
        "\n",
        "X_train = torch.Tensor(df_train_sc.to_numpy())\n",
        "X_test = torch.Tensor(df_test_sc.to_numpy())\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OiAVEGL4Zidn"
      },
      "outputs": [],
      "source": [
        "train, test = TensorDataset(X_train, y_train), TensorDataset(X_test, y_test)\n",
        "train_DataLoader= DataLoader(train, batch_size=batch, shuffle=True)\n",
        "test_DataLoader = DataLoader(test, batch_size=batch, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Mb5g5J6rZigC"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    \n",
        "    self.generator = nn.Sequential(\n",
        "      nn.Linear(100, 128),\n",
        "      nn.Linear(128, 256),\n",
        "      nn.ReLU(True),\n",
        "      nn.Linear(256, 512),\n",
        "      nn.ReLU(True),\n",
        "      nn.Linear(512, 39)\n",
        "    )\n",
        "    \n",
        "    self.generator.apply(self.__init_weights)\n",
        "\n",
        "  def forward(self, z, y):\n",
        "    y = F.one_hot(y, num_classes=10)\n",
        "    \n",
        "    z = torch.cat((z, y), 1)\n",
        "    x = self.generator(z)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  def __init_weights(self,m):\n",
        "    #Init the weights (optional)\n",
        "    if type(m) == nn.Linear:\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.01)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CRN9vsJiZiiQ"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "  def __init__(self, h_dim=128):\n",
        "    super(Discriminator, self).__init__()\n",
        "    \n",
        "    self.discriminator = nn.Sequential(\n",
        "      nn.Linear(49, 512),\n",
        "      nn.Linear(512, 256),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      nn.Linear(256, 128),\n",
        "      nn.LeakyReLU(inplace=True),\n",
        "      nn.Linear(128, 1),\n",
        "      nn.Sigmoid()\n",
        "    )\n",
        "    \n",
        "    self.discriminator.apply(self.__init_weights)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    y = F.one_hot(y, num_classes=10)\n",
        "    \n",
        "    x = torch.cat((x, y), 1)\n",
        "    x = self.discriminator(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  def __init_weights(self,m):\n",
        "    if type(m) == nn.Linear:\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lOpTZhVpZik5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def positiveLoss(output):\n",
        "    \n",
        "    y = torch.FloatTensor( output.size(0)).uniform_(0.9, 1).to(device)\n",
        "    loss = nn.BCELoss()(output.squeeze(), y)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ARB8ky03Zind"
      },
      "outputs": [],
      "source": [
        "def FalseLoss(output):\n",
        "    y = torch.FloatTensor(output.size(0)).uniform_(0, 0.1).to(device) \n",
        "    loss = nn.BCELoss()(output.squeeze(), y)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xi-iIswXZiqi"
      },
      "outputs": [],
      "source": [
        "lr = 5e-4\n",
        "Gen = Generator().to(device)\n",
        "Dis = Discriminator().to(device)\n",
        "Gen_opt = torch.optim.SGD(Gen.parameters(), lr=lr)\n",
        "Dis_opt = torch.optim.SGD(Dis.parameters(), lr=lr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZCD6V9bX-aS",
        "outputId": "6ce69d94-ed95-42ca-a884-5accaefb952e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 , d_loss: 1.7360 | g_loss: 0.7758\n",
            "epoch 2 , d_loss: 1.2128 | g_loss: 0.7685\n",
            "epoch 3 , d_loss: 0.9504 | g_loss: 0.8929\n",
            "epoch 4 , d_loss: 1.2400 | g_loss: 1.1020\n",
            "epoch 5 , d_loss: 0.9983 | g_loss: 1.2528\n",
            "epoch 6 , d_loss: 0.7344 | g_loss: 1.1920\n",
            "epoch 7 , d_loss: 1.1000 | g_loss: 1.3653\n",
            "epoch 8 , d_loss: 1.0416 | g_loss: 1.5756\n",
            "epoch 9 , d_loss: 1.1677 | g_loss: 1.3380\n",
            "epoch 10 , d_loss: 1.0651 | g_loss: 1.3616\n",
            "epoch 11 , d_loss: 1.0108 | g_loss: 1.4940\n",
            "epoch 12 , d_loss: 1.0013 | g_loss: 1.5678\n",
            "epoch 13 , d_loss: 0.7823 | g_loss: 1.5905\n",
            "epoch 14 , d_loss: 0.8193 | g_loss: 1.5621\n",
            "epoch 15 , d_loss: 0.6519 | g_loss: 1.5199\n",
            "epoch 16 , d_loss: 0.7749 | g_loss: 1.5705\n",
            "epoch 17 , d_loss: 1.1557 | g_loss: 1.5877\n",
            "epoch 18 , d_loss: 0.9515 | g_loss: 1.7632\n",
            "epoch 19 , d_loss: 0.9055 | g_loss: 1.4526\n",
            "epoch 20 , d_loss: 1.5029 | g_loss: 1.6936\n",
            "epoch 21 , d_loss: 0.9462 | g_loss: 1.4911\n",
            "epoch 22 , d_loss: 0.9276 | g_loss: 1.8217\n",
            "epoch 23 , d_loss: 0.7071 | g_loss: 1.7590\n",
            "epoch 24 , d_loss: 0.8243 | g_loss: 1.8636\n",
            "epoch 25 , d_loss: 0.6415 | g_loss: 1.4404\n",
            "epoch 26 , d_loss: 1.0606 | g_loss: 1.4850\n",
            "epoch 27 , d_loss: 0.8124 | g_loss: 1.5908\n",
            "epoch 28 , d_loss: 0.9781 | g_loss: 1.8356\n",
            "epoch 29 , d_loss: 0.7561 | g_loss: 1.9571\n",
            "epoch 30 , d_loss: 0.8517 | g_loss: 1.6562\n",
            "epoch 31 , d_loss: 0.7488 | g_loss: 1.5332\n",
            "epoch 32 , d_loss: 0.7174 | g_loss: 1.8437\n",
            "epoch 33 , d_loss: 1.2122 | g_loss: 1.3953\n",
            "epoch 34 , d_loss: 0.9877 | g_loss: 1.7662\n",
            "epoch 35 , d_loss: 0.7671 | g_loss: 1.7497\n",
            "epoch 36 , d_loss: 0.8168 | g_loss: 1.7969\n",
            "epoch 37 , d_loss: 1.4359 | g_loss: 1.7777\n",
            "epoch 38 , d_loss: 0.8179 | g_loss: 1.7379\n",
            "epoch 39 , d_loss: 0.6268 | g_loss: 1.9800\n",
            "epoch 40 , d_loss: 1.0521 | g_loss: 1.7005\n",
            "epoch 41 , d_loss: 1.2861 | g_loss: 1.3490\n",
            "epoch 42 , d_loss: 1.1962 | g_loss: 1.6176\n",
            "epoch 43 , d_loss: 0.7936 | g_loss: 1.6818\n",
            "epoch 44 , d_loss: 1.0716 | g_loss: 1.7697\n",
            "epoch 45 , d_loss: 1.0861 | g_loss: 1.6389\n",
            "epoch 46 , d_loss: 1.2584 | g_loss: 1.4792\n",
            "epoch 47 , d_loss: 1.2430 | g_loss: 1.8547\n",
            "epoch 48 , d_loss: 1.1577 | g_loss: 1.9341\n",
            "epoch 49 , d_loss: 0.8724 | g_loss: 1.6527\n",
            "epoch 50 , d_loss: 0.8128 | g_loss: 1.8291\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(30):\n",
        "\n",
        "    for batch_i, (X, y) in enumerate(train_DataLoader):\n",
        "        batch_size = X.size(0)\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, 90))\n",
        "        z = torch.from_numpy(z).float().to(device)\n",
        "        Gen_opt.zero_grad()\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        dReal = Dis(X, y)\n",
        "        DLoss = positiveLoss(dReal)\n",
        "        notReal_G = Gen(z, y)\n",
        "        notReal_D = Dis(notReal_G, y)\n",
        "        DLoss = DLoss + FalseLoss(notReal_D)\n",
        "        DLoss.backward()\n",
        "        Gen_opt.step()\n",
        "        Gen_opt.zero_grad()\n",
        "        notReal_G = Gen(z, y)\n",
        "        g_loss = positiveLoss(Dis(notReal_G, y))\n",
        "        g_loss.backward()\n",
        "        Gen_opt.step()\n",
        "            \n",
        "    print('epoch [{}/{}], d_loss: {:6.4f} | g_loss: {:6.4f}'.format(epoch + 1, num_epochs, d_loss.item(), g_loss.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clCVNsgyp-6o"
      },
      "source": [
        "**classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEW9OK_7bNuQ",
        "outputId": "e31af20a-2185-4240-a913-f52f2d25e674"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.05      0.97      0.09        90\n",
            "           2       0.56      0.35      0.43     19937\n",
            "           3       0.65      0.73      0.69     29645\n",
            "           4       0.86      0.89      0.88     17562\n",
            "           5       0.98      0.94      0.96     41997\n",
            "           6       1.00      1.00      1.00     56013\n",
            "           7       0.74      0.92      0.82      8483\n",
            "           8       0.63      0.45      0.53      1585\n",
            "           9       0.18      0.79      0.29        29\n",
            "\n",
            "    accuracy                           0.85    175341\n",
            "   macro avg       0.57      0.70      0.57    175341\n",
            "weighted avg       0.86      0.85      0.85    175341\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "## Random Forest Classifier\n",
        "clf_RF=RandomForestClassifier()\n",
        "clf_RF.fit(X_train.numpy(),y_train.numpy())\n",
        "y_pred=clf_RF.predict(X_test.numpy())\n",
        "print(classification_report(y_pred,y_test.numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDA5vn_TqrDq",
        "outputId": "70ca827e-1b40-4cd3-c686-296c3a62f924"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/interpret/glassbox/ebm/ebm.py:922: UserWarning: Multiclass is still experimental. Subject to change per release.\n",
            "  warn(\"Multiclass is still experimental. Subject to change per release.\")\n",
            "/usr/local/lib/python3.7/dist-packages/interpret/glassbox/ebm/ebm.py:925: UserWarning: Detected multiclass problem: forcing interactions to 0\n",
            "  warn(\"Detected multiclass problem: forcing interactions to 0\")\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.01      0.71      0.01        17\n",
            "           2       0.49      0.33      0.39     18473\n",
            "           3       0.68      0.69      0.68     32758\n",
            "           4       0.84      0.85      0.85     18006\n",
            "           5       0.98      0.99      0.98     39773\n",
            "           6       1.00      1.00      1.00     56000\n",
            "           7       0.75      0.85      0.80      9172\n",
            "           8       0.56      0.57      0.56      1118\n",
            "           9       0.11      0.58      0.18        24\n",
            "\n",
            "    accuracy                           0.84    175341\n",
            "   macro avg       0.54      0.66      0.55    175341\n",
            "weighted avg       0.85      0.84      0.84    175341\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Explainable Boosting Classifier\n",
        "clf_exp = ExplainableBoostingClassifier()\n",
        "clf_exp.fit(X_train.numpy(),y_train.numpy())\n",
        "y_pred=clf_exp.predict(X_test.numpy())\n",
        "print(classification_report(y_pred,y_test.numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psvBEr89rJ12"
      },
      "outputs": [],
      "source": [
        "##  Classical Neural Network\n",
        "\n",
        "class classicalNN(nn.Module):\n",
        "  def __init__(self, input_dim = 40, hid_dim= 30, output_dim=10):\n",
        "    super(classicalNN, self).__init__()\n",
        "    \n",
        "    self.clf = nn.Sequential(\n",
        "      nn.Linear(input_dim, hid_dim),\n",
        "      nn.ReLU(True),\n",
        "      nn.Linear(hid_dim, output_dim)\n",
        "    )\n",
        "    self.clf.apply(self.__init_weights)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.clf(x)\n",
        "    return output\n",
        "\n",
        "  def __init_weights(self,m):\n",
        "    #Init the weights (optional)\n",
        "    if type(m) == nn.Linear:\n",
        "      torch.nn.init.xavier_uniform_(m.weight)\n",
        "      m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "def train(model, dataset, epochs, learning_rate):\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in dataset:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "learning_rate = 5e-4\n",
        "epochs = 30\n",
        "\n",
        "clf_NN = classicalNN().to(device)\n",
        "train(clf_NN, train_DataLoader, epochs, learning_rate)\n",
        "clf_NN.eval()\n",
        "\n",
        "\n",
        "results = clf_NN(X_test.to(device))\n",
        "y_pred = torch.max(results.data, 1)[1].cpu().detach().numpy()\n",
        "\n",
        "print(classification_report(y_pred,y_test.numpy()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBtetLpsogDP",
        "outputId": "b16b4087-985d-486a-bb91-80bc61068c30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.11      0.27      0.16      5140\n",
            "           3       0.68      0.54      0.60     41915\n",
            "           4       0.09      0.55      0.16      3065\n",
            "           5       0.98      0.79      0.87     49881\n",
            "           6       0.90      0.67      0.77     75296\n",
            "           7       0.00      0.00      0.00        44\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.66    175341\n",
            "   macro avg       0.28      0.28      0.26    175341\n",
            "weighted avg       0.83      0.66      0.73    175341\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Regenerated Data"
      ],
      "metadata": {
        "id": "i7HU4wsVwlfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data={}\n",
        "train, target = [], []\n",
        "\n",
        "classes = np.unique(y_train, return_counts=True)\n",
        "maxInd=classes[0][classes[1]==np.amax(classes[1])][0]\n",
        "\n",
        "for elem in classes[0]:\n",
        "  s = classes[1][maxInd] - classes[1][elem]\n",
        "  y = torch.ones((s), dtype=torch.int64).to(device) * elem\n",
        "  x = np.random.uniform(-1, 1, size=(s, 90))\n",
        "  x = torch.from_numpy(x).float().to(device)\n",
        "  new_data[elem]= G(x, y).cpu().detach().numpy()\n",
        "\n",
        "for classes, elem in new_data.items():\n",
        "    train.append(elem)\n",
        "    target.append(np.ones(len(elem) * classes))\n",
        "\n",
        "xTrain = np.concatenate(train.append(X_train.numpy()))\n",
        "yTrain =  np.concatenate(target.append(y_train.numpy()))\n",
        "ind = np.random.permutation(len(xTrain))\n",
        "xTrain = torch.tensor(xTrain[ind])\n",
        "yTrain = torch.tensor( yTrain[ind], dtype=torch.int64)\n",
        "regen_dataLoder = DataLoader(TensorDataset(xTrain, yTrain), batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "KncYRE6bxDYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Random Forest Classifier\n",
        "clf_RF=RandomForestClassifier()\n",
        "clf_RF.fit(xTrain.numpy(), yTrain)\n",
        "y_pred=clf_RF.predict(X_test.numpy())\n",
        "print(classification_report(y_pred,y_test.numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLsBzVIJogHI",
        "outputId": "931afc1e-ba24-47de-b42b-e8b59823c750"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         0\n",
            "         1.0       0.05      0.96      0.10        97\n",
            "         2.0       0.55      0.34      0.42     19658\n",
            "         3.0       0.65      0.74      0.69     29611\n",
            "         4.0       0.86      0.87      0.87     18004\n",
            "         5.0       0.98      0.94      0.96     41913\n",
            "         6.0       1.00      1.00      1.00     56000\n",
            "         7.0       0.74      0.92      0.82      8426\n",
            "         8.0       0.65      0.46      0.54      1605\n",
            "         9.0       0.15      0.70      0.24        27\n",
            "\n",
            "    accuracy                           0.85    175341\n",
            "   macro avg       0.56      0.69      0.56    175341\n",
            "weighted avg       0.86      0.85      0.85    175341\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "POxELMoLrby2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AML_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}